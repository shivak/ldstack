import numpy as np
import tensorflow as tf
from linear_recurrent_net.linear_recurrent_net.tensorflow_binding import linear_recurrence

# Initialize with uniformly random complex eigenvalues of magnitude 1.
# If Î» has polar coordinates (r, Î¸) then ln(Î») = ln r + Î¸i
# Also, ln(Î»*) = ln r - Î¸i
# Fix r=1, i.e. ln r = 0. (For numerical reasons, can fix r=1-ğ›¿ for some small ğ›¿) 
# Note this only requires n/2 parameters rather than n
def unitary_ldstack_vars(m, n, k, scope):
  with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):
    if n % 2 != 0:
      raise "n must be even"

    half_n = round(n/2)
    Î¸_init = np.random.uniform(low=-np.pi, high=np.pi, size=[k,half_n]).astype(np.float32)
    Î¸ = tf.get_variable("eig_angle", initializer=tf.constant(Î¸_init), dtype=tf.float32, trainable=True)
    lnÎ»_r = tf.zeros((k,n), dtype=tf.float32)
    #lnÎ»_r = np.log(1-ğ›¿)*tf.ones((k,n), dtype=tf.float32)
    
    lnÎ»_i = tf.concat([Î¸, -Î¸], axis=1)
    lnÎ» = tf.complex(lnÎ»_r, lnÎ»_i)
    lnÎ»_init = 0 #FIXME

    C_init = np.random.uniform(low=-0.0001, high=0.0001, size=[k,m,n]).astype(np.float32)
    C = tf.get_variable("C", dtype=tf.float32, initializer=tf.constant(C_init), trainable=True)
    D_init = np.random.uniform(low=-0.001, high=0.001, size=[k,m]).astype(np.float32)
    D = tf.get_variable("D", dtype=tf.float32, initializer=tf.constant(D_init), trainable=True)
    Dâ‚’_init = np.random.uniform(low=-0.0000001, high=0.0000001, size=[m]).astype(np.float32)
    Dâ‚’ = tf.get_variable("D0", dtype=tf.float32, initializer=tf.constant(Dâ‚’_init), trainable=True)    
    
    return (lnÎ», C, D, Dâ‚’), (lnÎ»_init, C_init, D_init, Dâ‚’_init)

# Initialization as roots of monic polynomial with random coefficients
def ldstack_vars(m, n, k, scope, Î»_init=None, C_init=None, D_init=None, Dâ‚’_init=None):
  if Î»_init is None:
    Î»_init = np.zeros((k,n), dtype=np.complex64)
    for i in np.arange(k):
      Ainit = np.diagflat(np.ones(shape=n-1), 1).astype(np.float32)
      Ainit[-1,:] = np.random.normal(size=n) / n
      Î»_init[i] = np.linalg.eigvals(Ainit)
  Î»_init = Î»_init.flatten()
  
  with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):
    # isolated (non paired) log eigenvalue ends up real if imaginary part is either 0 or pi 
    # only optimize over real part, fix the imaginary part. this ensures eigenvalue is real, but fixes its sign
    # (which is OK, because it shouldn't approach zero anyway.)
    lnÎ»_real_init = np.log(Î»_init[np.isreal(Î»_init)] + 0j)
    lnÎ»_real_r = tf.get_variable("ln_eig_real_r", dtype=tf.float32, initializer=tf.constant(lnÎ»_real_init.real), trainable=True)
    lnÎ»_real = tf.complex(lnÎ»_real_r, lnÎ»_real_init.imag)

    comp_pair = (Î»_init[np.iscomplex(Î»_init)])[::2] # only get one part of conjugate pair.
    ln_comp_pair = np.log(comp_pair)
    lnÎ»_comp_init_a = ln_comp_pair.real.astype(np.float32)
    lnÎ»_comp_init_b = ln_comp_pair.imag.astype(np.float32)
            
    lnÎ»_comp_a = tf.get_variable("ln_eig_comp_a", dtype=tf.float32, initializer=tf.constant(lnÎ»_comp_init_a), trainable=True)
    lnÎ»_comp_b = tf.get_variable("ln_eig_comp_b", dtype=tf.float32, initializer=tf.constant(lnÎ»_comp_init_b), trainable=True)
    # Keep conjugate pairs adjacent [-b_1, b_1, -b_2, b_2, ...]
    # tf.repeat() not in 1.13
    lnÎ»_comp_r = tf.keras.backend.repeat_elements(lnÎ»_comp_a, 2, axis=0)
    lnÎ»_comp_i = tf.keras.backend.repeat_elements(lnÎ»_comp_b, 2, axis=0) * np.tile([-1,1], lnÎ»_comp_init_b.shape[0])
    lnÎ»_comp = tf.complex(lnÎ»_comp_r, lnÎ»_comp_i)
    # restore original order of eigenvalues
    # TF BUG in scatter_nd() seems to mangle complex values 
    if False:
      lnÎ» = tf.scatter_nd(
              np.concatenate((np.argwhere(np.isreal(Î»_init)), np.argwhere(np.iscomplex(Î»_init)))),
              tf.concat([lnÎ»_real, lnÎ»_comp], axis=0),
              [k*n])    
    else:
      lnÎ»_r = tf.scatter_nd(
                np.concatenate((np.argwhere(np.isreal(Î»_init)), np.argwhere(np.iscomplex(Î»_init)))),
                tf.real(tf.concat([lnÎ»_real, lnÎ»_comp], axis=0)),
                [k*n]) 
      lnÎ»_i = tf.scatter_nd(
                np.concatenate((np.argwhere(np.isreal(Î»_init)), np.argwhere(np.iscomplex(Î»_init)))),
                tf.imag(tf.concat([lnÎ»_real, lnÎ»_comp], axis=0)),
                [k*n]) 
      lnÎ» = tf.complex(lnÎ»_r, lnÎ»_i)
    lnÎ» = tf.reshape(lnÎ», [k,n], name="ln_eig")
    
    if C_init is None:
      C_init = np.random.uniform(low=-0.0000001, high=0.0000001, size=[k,m,n]).astype(np.float32)
    C = tf.get_variable("C", dtype=tf.float32, initializer=tf.constant(C_init), trainable=True)
    if D_init is None:
      D_init = np.random.uniform(low=-0.0000001, high=0.0000001, size=[k,m]).astype(np.float32)
    D = tf.get_variable("D", dtype=tf.float32, initializer=tf.constant(D_init), trainable=True)
    if Dâ‚’_init is None:
      Dâ‚’_init = np.random.uniform(low=-0.0000001, high=0.0000001, size=[m]).astype(np.float32)
    Dâ‚’ = tf.get_variable("D0", dtype=tf.float32, initializer=tf.constant(Dâ‚’_init), trainable=True)    
    return (lnÎ», C, D, Dâ‚’), (Î»_init, C_init, D_init, Dâ‚’_init) 

# x : [T, b, k]
# Î» : [k, n]
# C : [k, m, n]
# D : [k, m]
# Î± : [T, b, k, n] is:
#      Î±_0, ..., Î±_{T-1} (in "standard" mode)
#      Î±_1, ..., Î±_T     (otherwise)
# Returns (for t in [1,...,T]):
# sÊ¹: [T, b, k, n] is:
#     sÊ¹_t = Î±_{t-1}Â·Î»Â·sÊ¹_{t-1}  + Bx_{t-1}   (in "standard" mode)
#     sÊ¹_t = Î±_t    Â·Î»Â·sÊ¹_{t-1}  + Bx_t       (otherwise)
# currently with sÊ¹_0 = 0
# y : [T, b, k, m] is:
#     y_t  = CÊ¹sÊ¹_t + Dx_t
def batch_simo_lds(x, lnÎ», C, D, Dâ‚’, Î±=None, standard=False):
  k = x.shape[2]
  n = tf.shape(lnÎ»)[1]
  b = x.shape[1]
  T = x.shape[0]
  m = C.shape[1]

  # ratios[k,i,j] = Î»i / Î»j for k'th Î»
  Î» = tf.exp(lnÎ»)
  ratios = tf.reshape(Î», (k, -1, 1)) / tf.reshape(Î», (k, 1, -1))
  #ratios = tf.exp(tf.reshape(lnÎ», (k, -1, 1)) - tf.reshape(lnÎ», (k, 1, -1)))
  ratios = tf.linalg.set_diag(ratios, tf.zeros(shape=[k,n], dtype=tf.complex64))
  # BÊ¹_i = Î»i^{n-1} / âˆ_{iâ‰ j} Î»i-Î»j
  # log BÊ¹_i 
  # = (n-1) logÎ»i - âˆ‘_{iâ‰ j} log(Î»i-Î»j)
  # = (n-1) logÎ»i - âˆ‘_{iâ‰ j} logÎ»i + log(1 - Î»j/Î»i)
  # = -âˆ‘_{iâ‰ j} log(1 - Î»j/Î»i)
  # = -âˆ‘_j log(1 - ratios[k,j,i]) 
  # because log(1 - ratios[k,i,i]) = 0
  lnBÊ¹ = -1* tf.reduce_sum(tf.log(1.0 - ratios), axis=1)
  # BÊ¹ : [k, n]
  # BÊ¹Â·x : [T, b, k*n]
  BÊ¹ = tf.exp(lnBÊ¹)
  BÊ¹Â·x = tf.reshape(tf.expand_dims(x, -1)*BÊ¹, (T, b, k*n))

  # Î±Â·Î» : [T, b, k*n]
  # sÊ¹ : [T, b, k, n]  
  if Î± is None:
    Î±Â·Î» =  tf.tile(tf.reshape(Î», (1,1,-1)), (T, b, 1)) 
  else:
    Î±Â·Î» = Î±*tf.reshape(Î», (1, 1, k, n))  
    Î±Â·Î» = tf.reshape(Î±Â·Î», (T, b, k*n))
  # linear_recurrence computes sÊ¹_t = Î±_tÂ·Î»Â·sÊ¹_{t-1} + Bx_t
  # for standard LDS, we need to shift Î± and x
  # sÊ¹_t = Î±_{t-1}Â·Î»Â·sÊ¹_{t-1} + Bx_{t-1}
  if standard:
    Î±Â·Î»  = tf.concat([tf.zeros((1,b,k*n), dtype=tf.complex64),  Î±Â·Î»[:-1]], axis=0)
    BÊ¹Â·x = tf.concat([tf.zeros((1,b,k*n), dtype=tf.complex64),  BÊ¹Â·x[:-1]], axis=0)
  sÊ¹ = linear_recurrence(Î±Â·Î», BÊ¹Â·x)
  sÊ¹ = tf.reshape(sÊ¹, [T, b, k, n])
    
  # U_{i,j} = 1 / Î»j^{n-i}
  # log U_{i,j} = -(n-i) logÎ»j
  # let C be a row of 
  # log C'_{i,j}
  # = log <C_{i,:}, U_{:,j}>
  # = log sum_k exp(log C_{i,k} + log U_{k,j})
  lnC = tf.transpose(tf.log(tf.complex(C, 0.0)), (1,0,2)) # [m, k, n]
  powers = tf.cast(n - tf.range(1,n+1), tf.complex64)
  lnU = tf.reshape(-1*powers, (1,-1,1)) * tf.expand_dims(lnÎ», 1) # [1,n,1]*[k,1,n] -> [k, n, n] 
  sum_logs = tf.expand_dims(lnC,-1) + tf.expand_dims(lnU, 0) # [m,k,n,1]+[1,k,n,n] -> [m, k, n, n]
  CÊ¹ = tf.reduce_sum(tf.exp(sum_logs), axis=-2) # [m, k, n]
  CÊ¹ = tf.transpose(CÊ¹, (1,2,0)) # [k, n, m]
  
  CÊ¹Â·sÊ¹ = tf.real(tf.reduce_sum(tf.expand_dims(sÊ¹, -1)*tf.reshape(CÊ¹, (1,1,k,n,m)), axis=-2))
  DÂ·x = tf.real(tf.expand_dims(x, -1)) * tf.reshape(D, (1,1,k,m))
  y = CÊ¹Â·sÊ¹ + DÂ·x + Dâ‚’

  return sÊ¹, y

def recipsq(a):
  return tf.math.rsqrt(1 + tf.math.square(a))

# x: [T, batch_size, d] is complex (this is unusual, but matches the format of linear_recurrence.
#       And actually, is faster for GPU computation, and should be the standard for RNNs.)
# returns [T, batch_size, m] and params
# see batch_simo_lds for meaning of standard
def ldstack(x, n, m, k, Î”, scope, Î»_init=None, C_init=None, D_init=None, Dâ‚’_init=None, standard=True):
  with tf.variable_scope(scope, reuse=tf.AUTO_REUSE):
    T, b, d = x.shape

    if d > 1 or k > 1:
      R = tf.get_variable("R", dtype=tf.float32, initializer=tf.random_normal_initializer, shape=[d,k], trainable=False)
      R = tf.cast(R, tf.complex64)
      x = tf.tensordot(x, R, [[-1], [0]])
    
    #(lnÎ», C, D, Dâ‚’), (Î»_init, C_init, D_init, Dâ‚’_init) = ldstack_vars(m, n, k, "lds", Î»_init, C_init, D_init, Dâ‚’_init)
    (lnÎ», C, D, Dâ‚’), (Î»_init, C_init, D_init, Dâ‚’_init) = unitary_ldstack_vars(m, n, k, "lds")
    Î» = tf.exp(lnÎ»)
    # Î» : [k, n]
    # C : [k, m, n]
    # Î± : [T, b, k, n]
    # sÊ¹: [T, b, k, n]
    # y : [T, b, k, m]
    Î± = None
    for i in np.arange(Î”):
      sÊ¹, y = batch_simo_lds(x, lnÎ», C, D, Dâ‚’, Î±, standard)
      Î»Â·sÊ¹ = tf.reshape(Î», (1,1,k,n)) * sÊ¹
      Î± = recipsq(Î»Â·sÊ¹)

    y = tf.reduce_mean(y, axis=2)
    return y, (lnÎ», C, D, Dâ‚’), (Î»_init, C_init, D_init, Dâ‚’_init)
