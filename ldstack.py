import numpy as np
import tensorflow as tf
from linear_recurrent_net.linear_recurrent_net.tensorflow_binding import linear_recurrence, sparse_linear_recurrence
#from harold import controllability_matrix

'''
LDStack is somewhat unusual in that it supports multiple different underlying parameterizations of the 
optimization variables. A simple example is that we can optimize over C as an n-dimensional real vector,
or over C' = CU, an n-dimensional complex vector, or over log C'. These different parameterizations must be 
initialized in different manners. They "reconstitute" the actual quantities used by the model in different
ways, as well. 
Each parameterization is defined by two functions. 
1. An initialization function, which takes options for the parameterization, and 
returns a function which is called once the dimensions of the variables are known. 
That function, in turn, returns the underlying optimization variables, as well as
the reconstitution function, defined next.
Example:
   def unitary_eig_params(trainÎ»=True, useD=True):
     def params(n,m,k):
       ...
       return underlying, Î¸
     return params
2. A reconstitution function, which takes the underlying variables and produces the quantities used by 
the model. Example:
   def unitary_eig_constitute(Î¸):
     ...
     return lnÎ», Î»
'''

# Initialize with uniformly random complex eigenvalues of magnitude 1.
# If Î» has polar coordinates (r, Î¸) then ln(Î») = ln(r) + Î¸i
# Also, ln(Î»*) = ln(r) - Î¸i
# Fix r=1, i.e. ln(r) = 0. (For numerical reasons, can fix r=1-ğ›¿ for some small ğ›¿) 
# Note this only requires n/2 parameters rather than n
# Should constrain -Ï€ â‰¤ Î¸ â‰¤ Ï€
def log_polar_AB_param(train_ln_r=True, 
                       train_Î¸=True, 
                       ln_r_init=tf.keras.initializers.RandomUniform(minval=-10,maxval=0), 
                       Î¸_init=tf.keras.initializers.RandomUniform(-2*np.pi, 2*np.pi)):
  def params(n, m, k):
    if n % 2 != 0:
      raise "n must be even"
    half_n = round(n/2)

    ln_r_val = ln_r_init((k,half_n))
    Î¸_val = Î¸_init((k, half_n))
    ln_r = tf.Variable(ln_r_val,  name="eig_radius", dtype=tf.float32, trainable=train_ln_r)
    Î¸ = tf.Variable(Î¸_val, name="eig_angle",  dtype=tf.float32, trainable=train_Î¸)

    return (ln_r,Î¸), log_polar_AB_constitute
  return params
 

def log_polar_AB_constitute(ln_r, Î¸):
  k, half_n = ln_r.shape
  lnÎ»_r = tf.concat([ln_r, ln_r], axis=1)
  lnÎ»_i = tf.concat([Î¸, -Î¸], axis=1)
  lnÎ» = tf.complex(lnÎ»_r, lnÎ»_i)
  Î» = tf.exp(lnÎ»)
  return lnÎ», Î»

def unitary_AB_param():
  return log_polar_AB_param(train_ln_r=False, ln_r_init=tf.keras.initializers.Zeros())


# Let h be relu or similar "hinge" function. The two parameters (Î±, Ï‰) represent 
# two eigenvalues (either both real, or complex conjugate pairs) as: 
#   Î±      + h(-Ï‰) i
#   Î±+h(Ï‰) - h(-Ï‰) i
# Ï‰>0 makes the eigenvalues Î± and Î±+Ï‰ real
# Ï‰<0 makes the eigenvalues Î±Â±Ï‰i conjugate pairs
#
# We can also log space.
# If Î» = r exp(iÎ¸) (i.e. has polar coordinates (r, Î¸)) then ln(Î») = ln(r) + Î¸i
# Also, ln(Î»*) = ln(r) - Î¸i.
# Then we can repeat the hinging on ln(r) and Î¸ rather than re(Î») and im(Î»)

def hinged_AB_param(Î»_init=None, h=tf.keras.activations.relu, log=True): #elu(a, 0.1))):
  def params(n, m, k):
    if n % 2 != 0:
      raise "n must be even"
    half_n = int(n/2)

    if Î»_init is None:
      Î±_init = tf.keras.initializers.RandomUniform(-1, 1)((k,half_n))
      Ï‰_init = tf.keras.initializers.RandomUniform(-0.8, 0.8)((k,half_n))
    else:
      Î»_init_val = Î»_init(n,k) if callable(Î»_init) else Î»_init
      if log:
        # hack
        Î»_init_val = np.log(np.absolute(Î»_init_val)) + 1j*np.angle(Î»_init_val)

      Î±_init = np.zeros((k,half_n), dtype=np.float32)
      Ï‰_init = np.zeros((k,half_n), dtype=np.float32)
      
      for i in np.arange(k):
        Î»_init_f = Î»_init_val[i]
        # couple of real eigs represented as Î± and Î±+Ï‰ where Ï‰>0 
        real_Î» = np.real(Î»_init_f[np.isreal(Î»_init_f)])
        real_Î± = np.minimum(real_Î»[::2], real_Î»[1::2])
        real_Ï‰ = np.maximum(real_Î»[::2], real_Î»[1::2]) - real_Î±

        # complex eig pair represented as Î± Â± Ï‰i
        comp_Î»_pair = Î»_init_f[np.iscomplex(Î»_init_f)][::2] # assumes conj pairs are adjacent
        comp_Î± = np.real(comp_Î»_pair)
        comp_Ï‰ = -np.abs(np.imag(comp_Î»_pair))

        Î±_init[i] = np.concatenate([real_Î±, comp_Î±], axis=0)
        Ï‰_init[i] = np.concatenate([real_Ï‰, comp_Ï‰], axis=0)
              
    Î± = tf.Variable(Î±_init, dtype=tf.float32, name="alpha")
    Ï‰ = tf.Variable(Ï‰_init, dtype=tf.float32, name="omega")

    return (Î±,Ï‰,h,log), hinged_AB_constitute
  return params

def hinged_AB_constitute(Î±, Ï‰, h, log):
  r = tf.concat([Î±, Î± + h(Ï‰)], axis=1)
  i = tf.concat([h(-Ï‰), -h(-Ï‰)], axis=1)
  if log:
    lnÎ» = tf.complex(r, i)
    Î» = tf.math.exp(lnÎ»)
  else:
    Î» = tf.complex(r, i)
    lnÎ» = tf.math.log(Î»)
  return lnÎ», Î»

# Initialization as roots of monic polynomial with random coefficients
def randroot_Î»_init(n, k, max_radius=1.0):
  Î»_init = np.zeros((k,n), dtype=np.complex64)
  for i in np.arange(k):
    Ainit = np.diagflat(np.ones(shape=n-1), 1).astype(np.float32)
    Ainit[-1,:] = np.random.normal(size=n) / n
    Î»_init[i] = np.linalg.eigvals(Ainit)

  if max_radius is not None:
    Î»_init = Î»_init / np.maximum(max_radius, np.abs(Î»_init))

  return Î»_init

def safe_randroot_Î»_init(n, k, scale=4):  
  return randroot_Î»_init(scale*n, k)[:,:n]

# Initialization on Chebyshev nodes
def chebyshev_Î»_init(n, k, kind='first'):
  f = np.polynomial.chebyshev.chebpts1 if kind == 'first' else np.polynomial.chebyshev.chebpts2
  p = f(n)
  return np.tile(np.expand_dims(p, 0), (k, 1)).astype(np.complex64)


def log_AB_param(Î»_init):
  Î»_init_val = Î»_init # Python local variable strangeness
  def params(n,m,k):
    Î»_init = Î»_init_val.flatten()
    
    # isolated (unpaired) log eigenvalue ends up real if imaginary part is either 0 or pi 
    # only optimize over real part, fix the imaginary part. this ensures eigenvalue is real, but fixes its sign
    # (which is OK, because it shouldn't approach zero anyway.)
    lnÎ»_real_init = np.log(Î»_init[np.isreal(Î»_init)] + 0j)
    lnÎ»_real_r = tf.Variable(lnÎ»_real_init.real, name="ln_eig_real_r", dtype=tf.float32)

    comp_pair = (Î»_init[np.iscomplex(Î»_init)])[::2] # only get one part of conjugate pair.
    ln_comp_pair = np.log(comp_pair)
    lnÎ»_comp_init_a = ln_comp_pair.real.astype(np.float32)
    lnÎ»_comp_init_b = ln_comp_pair.imag.astype(np.float32)
              
    lnÎ»_comp_a = tf.Variable(lnÎ»_comp_init_a, name="ln_eig_comp_a", dtype=tf.float32)
    lnÎ»_comp_b = tf.Variable(lnÎ»_comp_init_b, name="ln_eig_comp_b", dtype=tf.float32)
    
    where_Î»_init_r = np.argwhere(np.isreal(Î»_init))
    where_Î»_init_i = np.argwhere(np.iscomplex(Î»_init))

    return (lnÎ»_real_r, lnÎ»_real_init.imag, lnÎ»_comp_a, lnÎ»_comp_b, where_Î»_init_r, where_Î»_init_i, k, n), log_AB_constitute
  return params

def log_AB_constitute(lnÎ»_real_r, lnÎ»_real_i_init, lnÎ»_comp_a, lnÎ»_comp_b, where_Î»_init_r, where_Î»_init_i, k, n):
  lnÎ»_real = tf.complex(lnÎ»_real_r, lnÎ»_real_i_init)

  # Keep conjugate pairs adjacent [-b_1, b_1, -b_2, b_2, ...]
  lnÎ»_comp_r = tf.repeat(lnÎ»_comp_a, 2, axis=0)
  lnÎ»_comp_i = tf.repeat(lnÎ»_comp_b, 2, axis=0) * np.tile([-1,1], lnÎ»_comp_b.shape[0])
  lnÎ»_comp = tf.complex(lnÎ»_comp_r, lnÎ»_comp_i)
  # restore original order of eigenvalues
  lnÎ» = tf.scatter_nd(
              np.concatenate((where_Î»_init_r, where_Î»_init_i)),
              tf.concat([lnÎ»_real, lnÎ»_comp], axis=0),
              [k*n])    
  lnÎ» = tf.reshape(lnÎ», [k,n])
  Î» = tf.exp(lnÎ»)

  return lnÎ», Î»

def standard_AB_param(Î»_init):
  Î»_init_val = Î»_init # Python local variable strangeness
  def params(n,m,k):
    Î»_init = Î»_init_val.flatten()
    
    Î»_real_init = Î»_init[np.isreal(Î»_init)]
    Î»_real_r = tf.Variable(Î»_real_init.real, name="eig_real_r", dtype=tf.float32)

    comp_pair = (Î»_init[np.iscomplex(Î»_init)])[::2] # only get one part of conjugate pair.
    Î»_comp_init_a = comp_pair.real.astype(np.float32)
    Î»_comp_init_b = comp_pair.imag.astype(np.float32)
              
    Î»_comp_a = tf.Variable(Î»_comp_init_a, name="eig_comp_a", dtype=tf.float32)
    Î»_comp_b = tf.Variable(Î»_comp_init_b, name="eig_comp_b", dtype=tf.float32)
    
    where_Î»_init_r = np.argwhere(np.isreal(Î»_init))
    where_Î»_init_i = np.argwhere(np.iscomplex(Î»_init))

    return (Î»_real_r, Î»_comp_a, Î»_comp_b, where_Î»_init_r, where_Î»_init_i, k, n), standard_AB_constitute
  return params

def standard_AB_constitute(Î»_real_r, Î»_comp_a, Î»_comp_b, where_Î»_init_r, where_Î»_init_i, k, n):
  Î»_real = tf.complex(Î»_real_r, 0.0)

  # Keep conjugate pairs adjacent [-b_1, b_1, -b_2, b_2, ...]
  Î»_comp_r = tf.repeat(Î»_comp_a, 2, axis=0)
  Î»_comp_i = tf.repeat(Î»_comp_b, 2, axis=0) * np.tile([-1,1], Î»_comp_b.shape[0])
  Î»_comp = tf.complex(Î»_comp_r, Î»_comp_i)
  # restore original order of eigenvalues
  Î» = tf.scatter_nd(
              np.concatenate((where_Î»_init_r, where_Î»_init_i)),
              tf.concat([Î»_real, Î»_comp], axis=0),
              [k*n])    
  Î» = tf.reshape(Î», [k,n])
  lnÎ» = tf.math.log(Î»)

  return lnÎ», Î»

def canonical_AB_param(a_stddev = 0.1):
  def params(n,m,k):
    a = tf.Variable(tf.random.normal((k,n), stddev=a_stddev) / float(n), name="a", dtype=tf.float32)
    A = np.diagflat(np.ones(shape=n-1), 1)[:-1]
    A = np.tile(A.reshape(1,n-1,n), (k,1,1))
    A = A.astype(np.float32)
    return (a,A), canonical_AB_constitute
  return params

def canonical_AB_constitute(a, A):
  a_ = tf.expand_dims(a, 1)
  A = tf.concat([A, -a_], axis=1)

  Î», _ = tf.linalg.eig(A) #FIXME: double work?
  lnÎ» = tf.math.log(Î»)
  return lnÎ», Î»

# Used for naive variant, which just has plain real eigenvalues
def naive_AB_param():
  def params(n,m,k):
    Î» = tf.Variable(tf.keras.initializers.GlorotNormal()((k,n)), dtype=tf.float32)
    return (Î»,), naive_AB_constitute
  return params

def naive_AB_constitute(Î»):
  Î» = tf.complex(Î», 0.0)
  lnÎ» = tf.math.log(Î»)
  return lnÎ», Î»

# Optimize over real C, potentially encountering numerical stability
def standard_C_param(C_init=None, C_stddev=0.00001):
  def params(n,m,k):
    C = tf.Variable(tf.random.normal((k,n,m), stddev=C_stddev), name="C", dtype=tf.float32)
    return (C,), standard_C_constitute
  return params

def standard_C_constitute(lnÎ», Î», C):
  return C, None


# Expands optimization over real C to complex CÊ¹.
# After optimization, CU â‰ˆ CÊ¹ with equivalent loss can be found 
# Furthermore, optimize over lnCÊ¹ for numerical reasons
# (typically, C is very close to 0)
# Uses lnÎ», Î» for initial value

def relaxed_C_param(CÊ¹_mean=0.0, CÊ¹_stddev=1):
  def params(n,m,k):
    CÊ¹_r = tf.Variable(tf.random.normal((k,n,m), mean=CÊ¹_mean, stddev=CÊ¹_stddev), name="C'_r", dtype=tf.float32)
    CÊ¹_i = tf.Variable(tf.random.normal((k,n,m), mean=CÊ¹_mean, stddev=CÊ¹_stddev), name="C'_i", dtype=tf.float32)
    return (CÊ¹_r, CÊ¹_i), relaxed_C_constitute
  return params

def relaxed_C_constitute(lnÎ», Î», CÊ¹_r, CÊ¹_i):
  CÊ¹ = tf.complex(CÊ¹_r, CÊ¹_i)
  return None, CÊ¹

def relaxed_log_C_param(CÊ¹_mean=1.0, CÊ¹_stddev=0.01):
  def params(n,m,k):
    lnCÊ¹_init = tf.math.log(tf.cast(tf.random.normal((k,n,m), mean=CÊ¹_mean, stddev=CÊ¹_stddev), tf.complex64))
    lnCÊ¹_r = tf.Variable(tf.math.real(lnCÊ¹_init), name="lnC'_r", dtype=tf.float32)
    lnCÊ¹_i = tf.Variable(tf.math.imag(lnCÊ¹_init), name="lnC'_i", dtype=tf.float32)
    return (lnCÊ¹_r, lnCÊ¹_i), relaxed_log_C_constitute
  return params

def relaxed_log_C_constitute(lnÎ», Î», lnCÊ¹_r, lnCÊ¹_i):
  lnCÊ¹ = tf.complex(lnCÊ¹_r, lnCÊ¹_i)
  CÊ¹ = tf.exp(lnCÊ¹)
  return None, CÊ¹

def standard_D_param(D_init=None, Dâ‚’_init=None, useD=True):
  def params(n,m,d):
    if useD:
      D = tf.Variable(
          np.random.uniform(low=-0.001, high=0.001, size=[d,m]).astype(np.float32) / m if D_init is None else D_init,
          name="D", dtype=tf.float32)
      Dâ‚’ = tf.Variable(
          0.0 if Dâ‚’_init is None else Dâ‚’_init, 
          name="D0", dtype=tf.float32)  
    else:
      D = tf.zeros((d,m), dtype=tf.float32)
      Dâ‚’ = 0.0
    return (D, Dâ‚’), lambda D, Dâ‚’: (D,Dâ‚’)
  return params

  
# Batch of d SIMO LDS, only returning last state. Uses much more memory-efficient SparseLinearRecurrence op
class SparseLDS(tf.keras.layers.Layer):
  def __init__(self, n, m, AB_param, C_param, D_param, standard=False):
    super(SparseLDS, self).__init__()
    self.lnÎ»_Î»_underlying, self.constitute_lnÎ»_Î» = AB_param(n, m, k)
    self.CÊ¹_underlying, self.constitute_CÊ¹ = C_param(n, m, k)
    self.D_Dâ‚’_underlying, self.constitute_D_Dâ‚’ = D_param(n, m, k)
    self.d = 1
    self.n = n
    self.m = m 
    self.standard = standard

  def call(self, x):
    b, T, d = x.shape
    n = self.n
    lnÎ», Î» = self.constitute_lnÎ»_Î»(*self.lnÎ»_Î»_underlying)
    _, CÊ¹ = self.constitute_CÊ¹(*((lnÎ», Î») + self.CÊ¹_underlying))
    D, Dâ‚’ = self.constitute_D_Dâ‚’(*self.D_Dâ‚’_underlying)
    BÊ¹ = tf.ones([1,n], dtype=tf.complex64)

    x = tf.complex(tf.transpose(x, (1,0,2)), 0.0)

    # linear_recurrence computes sÊ¹_t = Î»Â·sÊ¹_{t-1} + Bx_t
    # for standard LDS, we need to shift x
    # sÊ¹_t = Î»Â·sÊ¹_{t-1} + Bx_{t-1}
    if self.standard:
      x = tf.concat([tf.zeros((1,b,d), dtype=tf.complex64),  x[:-1]], axis=0)
    sâ‚œÊ¹ = sparse_linear_recurrence(Î», x, BÊ¹) 
    # sâ‚œÊ¹: [b, 1, n]
    # CÊ¹: [1, n, m]    
    # D : [1, m]
    # [b, 1, n] * [1, n, m] -> [b, m]
    CÊ¹Â·sâ‚œÊ¹ = tf.matmul(tf.squeeze(sâ‚œÊ¹), tf.squeeze(CÊ¹))
    DÂ·xâ‚œ = tf.matmul(x[-1], tf.complex(D, 0.0))
    yâ‚œ = CÊ¹Â·sâ‚œÊ¹ + DÂ·xâ‚œ + tf.complex(Dâ‚’, 0.0)
    
    return tf.math.real(yâ‚œ)

  # TODO: (A, B, C, D) by computing elementary symmetric polynomials 
  def canonical_lds():
    return None

'''
A MIMO LDS in (perturbed) Luenberger canonical form
n/d SIMO LDS, each of size d, with inputs coupled by a matrix E
'''
class LuenbergerLDS(tf.keras.layers.Layer):
  def __init__(self, n, m, AB_param, C_param, D_param, E_init=None, last_only=False):
    super(LuenbergerLDS, self).__init__()
    self.n = n
    self.m = m

    # Optimization for sparse case
    if last_only:
      self.lds = SparseLDS(n, m, AB_param, C_param, D_param)
    else:
      self.lds = LuenbergerStack(n, self.m, 1, AB_param, C_param, D_param, last_only=last_only)
    self.E = tf.keras.layers.Dense(use_bias=False, kernel_initializer=E_init)

  def build(self, input_shape):
    _, T, d = input_shape
    if self.n % d != 0:
      raise "n (LDS state size) must be divisible by d (input dimension)"  

  def call(self, x):
    EÂ·x = self.E(x)
    return self.lds(EÂ·x)


# Takes [b, T, d] real
# Returns [b, T, m] real
# Note: requires a fixed batch size. This means you must:
# 1. Use an InputLayer in the Keras model specification. 
# 2. Pass a TF dataset which has been batched with drop_remainder=True to model.fit(), 
#    not a NumPy array. (Otherwise the last batch will be the wrong size.)
class LDStack(tf.keras.layers.Layer):
  # n: state size
  # m: output size
  # Î”: depth of stack
  # standard: whether to compute 
  # last_only: return just the last element of the output sequence
  def __init__(self, n, m, Î”, AB_param, C_param, D_param, 
               ğ’¯_init=tf.keras.initializers.GlorotNormal(), 
               W_init=tf.keras.initializers.GlorotNormal(),
               Ï=tf.keras.activations.elu, 
               mode='standard', 
               last_only=False, 
               relaxV=True,
               r=1):
    super(LDStack, self).__init__()
    self.AB_param = AB_param
    self.C_param = C_param
    self.D_param = D_param
    self.ğ’¯_init = ğ’¯_init
    self.W_init = W_init
    self.n = n
    self.m = m 
    self.Î” = Î”
    if mode != 'standard' and mode != 'adjoint' and mode != 'naive':
      raise ValueError("mode must be standard, adjoint, or naive")
    self.mode = mode
    self.last_only = last_only
    self.relaxV = relaxV
    # TODO: fuse for common activations
    self.ğ›¿ = lambda a: Ï(a) - a
    self.r = r    
    self.lnÎ»_Î»_underlying, self.constitute_lnÎ»_Î» = self.AB_param(n, m, 1)

  def build(self, input_shape):
    _, _, self.d = input_shape

    # perhaps move to Luenberger? 
    self.C_CÊ¹_underlying, self.constitute_C_CÊ¹ = self.C_param(int(self.n/self.d), self.m, self.d)
    self.D_Dâ‚’_underlying, self.constitute_D_Dâ‚’ = self.D_param(self.n, self.m, self.d)
    if self.d > 1:
      self.E = tf.Variable(tf.random.normal((self.d,self.r), stddev=1/np.sqrt(self.n)), name="E", dtype=tf.float32, trainable=False)

    if self.mode != 'naive':
      if self.relaxV:
        if self.d == 1:
          self.W_r = tf.Variable(self.W_init((self.n, self.n)), dtype=tf.float32, name="W_r")
          self.W_i = tf.Variable(tf.keras.initializers.Zeros()((self.n, self.n)), dtype=tf.float32, name="W_i") 
        else:
          self.W_r = tf.Variable(self.W_init((self.n, self.n, self.d)), dtype=tf.float32, name="W_r")
          self.W_i = tf.Variable(tf.keras.initializers.Zeros()((self.n, self.n, self.d)), dtype=tf.float32, name="W_i")
      else:
        if self.d == 1:
          self.ğ’¯_r = tf.Variable(self.ğ’¯_init((self.n, self.n)), dtype=tf.float32, name="T_r")
          self.ğ’¯_i = tf.Variable(tf.keras.initializers.Zeros()((self.n, self.n)), dtype=tf.float32, name="T_i")        
          #self.ğ’¯__ = tf.Variable(self.ğ’¯_init((self.n, self.n)), dtype=tf.float32, name="T")
        # WEIRD: self.ğ’¯ and self.T are treated the same by Python
        else:
          self.ğ’¯_ = tf.Variable(self.ğ’¯_init((self.n, self.n, self.d)), dtype=tf.float32, name="T")

  # sÊ¹ = Vs in standard mode
  # sÊ¹ = V^{-H}s in adjoint mode
  # sÊ¹ : [T,b,n]
  # Î»  : [k,n]
  def s_from_sÊ¹(self, sÊ¹, Î»):
    # FIXME: eliminate tiling
    # FIXME: k and b
    T,b,r,n = sÊ¹.shape
    sÊ¹ = tf.reshape(sÊ¹, (T,b*r,n))    
    Î» = tf.tile(Î», (sÊ¹.shape[1], 1))
    if self.mode == 'standard':
      s = sÊ¹
      #s = vander_solve(Î», sÊ¹)
      tf.debugging.check_numerics(tf.math.real(s), "after vander solve")
    elif self.mode == 'adjoint':
      s = vanderh_solve(Î», sÊ¹)
    elif self.mode == 'naive':
      s = sÊ¹
    s = tf.reshape(s, (T,b,r,n))  
    return tf.math.real(s)
  def sÊ¹_from_s(self, s, Î»):
    s = tf.complex(s, 0.0)
    T,b,r,n = s.shape
    s = tf.reshape(s, (T,b*r,n))
    # FIXME: eliminate tiling
    Î» = tf.tile(Î», (s.shape[1], 1))
    if self.mode == 'standard':
      sÊ¹ = s
      #sÊ¹ = vander_mul(Î», s)
    elif self.mode == 'adjoint':
      sÊ¹ = vanderh_mul(Î», s)
    elif self.mode == 'naive':
      sÊ¹ = s
    sÊ¹ = tf.reshape(sÊ¹, (T,b,r,n))  
    return sÊ¹
  
  def ğ’¯j(self):
    ğ’¯_ = tf.complex(self.ğ’¯_r, self.ğ’¯_i)[...,None] if self.d == 1 else self.ğ’¯_
    # [1,1,r,d].[n,n,d,1]. -> [r,n,n]
    ğ’¯E = tf.matmul(self.E[None,None,...], ğ’¯_[...,None,:], transpose_a=True)[...,0] if self.d > 1 else ğ’¯_
    return tf.transpose(ğ’¯E, (2,0,1))
  # h = s in naive mode 
  # s = ğ’¯h otherwise
  def h_from_s(self, s, avg=False):
    if self.mode == 'naive':
      return s
    else:
      # FIXME: need linear system solving broadcasting
      ğ’¯jinv = tf.linalg.inv(self.ğ’¯j())
      # [1,1,r,n,n].[T,b,r,n,1] -> [T,b,r,n]
      H = tf.matmul(ğ’¯jinv[None,None,...], s[...,None])[...,0]
      return tf.reduce_mean(H, axis=-2) if avg else H
  def s_from_h(self, h):
    if self.mode == 'naive':
      return h
    else:
      #[1,1,r,n,n].[T,b,r,n,1] -> [T,b,r,n]
      return tf.matmul(self.ğ’¯j()[None,None,...],h[...,None])[...,0]

  def Wj(self):
    W = tf.complex(self.W_r, self.W_i)[...,None] if self.d == 1 else tf.complex(self.W_r, self.W_i)
    WE = tf.matmul(tf.complex(self.E[None,None,...], 0.0), W[...,None], transpose_a=True)[...,0] if self.d > 1 else W
    return tf.transpose(WE, (2,0,1))
  def direct_h_from_sÊ¹(self, sÊ¹, avg=False): 
    # FIXME: need linear system solving broadcasting
    Wjinv = tf.linalg.inv(self.Wj())
    # [1,1,r,n,n].[T,b,r,n,1] -> [T,b,r,n]
    H = tf.matmul(Wjinv[None,None,...], sÊ¹[...,None])[...,0]
    h = tf.reduce_mean(H, axis=-2) if avg else H    
    return tf.math.real(h)
  def direct_sÊ¹_from_h(self, h):
    return tf.matmul(self.Wj()[None,None,...],tf.complex(h[...,None], 0.0))[...,0]

  # BÊ¹ = 1s in standard mode
  # BÊ¹_i = Î»i^{n-1} / âˆ_{iâ‰ j} Î»i-Î»j in adjoint mode
  # BÊ¹ : [k, n]
  def BÊ¹(self, lnÎ», Î»):
    if self.mode == 'standard':
      return tf.ones_like(Î»)
    else:
      k, n = lnÎ».shape 
      # ratios[k,i,j] = Î»i / Î»j for k'th Î»
      ratios = tf.reshape(Î», (k, -1, 1)) / tf.reshape(Î», (k, 1, -1))
      ratios = tf.linalg.set_diag(ratios, tf.zeros(shape=[k,n], dtype=tf.complex64))

      # log BÊ¹_i 
      # = (n-1) logÎ»i - âˆ‘_{iâ‰ j} log(Î»i-Î»j)
      # = (n-1) logÎ»i - âˆ‘_{iâ‰ j} logÎ»i + log(1 - Î»j/Î»i)
      # = -âˆ‘_{iâ‰ j} log(1 - Î»j/Î»i)
      # = -âˆ‘_j log(1 - ratios[k,j,i]) 
      # because log(1 - ratios[k,i,i]) = 0
      lnBÊ¹ = -1* tf.reduce_sum(tf.math.log1p(-ratios), axis=1)
      BÊ¹ = tf.exp(lnBÊ¹)
      return BÊ¹

  def corrections(self, sÊ¹, Î», x):
    Î»Â·sÊ¹_1x = sÊ¹ * Î»[0] + x[...,None]
    if self.relaxV:
      AÌ…h_BÌ…x = self.direct_h_from_sÊ¹(Î»Â·sÊ¹_1x, avg=True)
    else:
      Ah_Bx = self.s_from_sÊ¹(Î»Â·sÊ¹_1x, Î»)
      AÌ…h_BÌ…x = self.h_from_s(Ah_Bx, avg=True)

    # cÊ¹_t = ğ›¿(AÌ…h_t + BÌ…x_t)
    # linear_recurrence computes sÊ¹_t = Î»Â·sÊ¹_{t-1} + 1x_t + cÊ¹_t
    # Suppose cÊ¹_t for t  = {0,...,T-1} in code is actually cÊ¹_t for t  = {1,...,T} in math
    # sÊ¹_t for t = {0,...,T-1} in code is actually sÊ¹_{t+1} for t = {1, ... ,T} in math
    # In math, for t = {1,...,T}
    #   h_{t+1} = Ah_t + Bx_t + c_t
    #   Ah_t + Bx_t = h_{t+1} - c_t
    # Corresponding to code, for t = {1,...,T}:
    #   h_t - c_t
    #AÌ…h_BÌ…x = h - (c if c is not None else 0.0)
    k = self.ğ›¿(AÌ…h_BÌ…x)
    #print(
    #  tf.reduce_mean(tf.linalg.norm(Î»Â·sÊ¹_1x, axis=-1)),
    #  tf.reduce_mean(tf.linalg.norm(AÌ…h_BÌ…x, axis=-1)), 
    #  tf.reduce_mean(tf.linalg.norm(k, axis=-1)), "corrections")    
    #tf.debugging.check_numerics(k, "corrections")
    # [T,b,n] -> [T,b,r,n]
    if self.relaxV:
      kÊ¹ = self.direct_sÊ¹_from_h(k[...,None,:])
    else:
      kÊ¹ = self.sÊ¹_from_s(self.s_from_h(k[...,None,:]), Î»)
    return kÊ¹

  # x: [b, T, d] if not time_major, else [T, b, d] 
  # linear_recurrence computes sÊ¹_t = Î»Â·sÊ¹_{t-1} + BÊ¹x_t + cÊ¹_t
  # currently with SÊ¹_0 = 0  
  # Suppose cÊ¹_t for t  = {0,...,T-1} in code is actually cÊ¹_t for t  = {1,...,T} in math
  # sÊ¹_t for t = {0,...,T-1} in code is actually sÊ¹_{t+1} for t = {1, ... ,T} in math
  def call(self, x):
    b,T,_ = x.shape
    # TODO: eliminate wasteful transpose
    x = tf.transpose(x, (1,0,2))

    if self.mode != 'naive' and self.d > 1:
      # [T,b,1,d].[1,1,d,r] -> [T,b,r]
      EÂ·x = tf.complex(tf.matmul(x[...,None,:], self.E[None,None,...], ), 0.0)[...,0,:]
    else:
      EÂ·x = tf.complex(x, 0.0)

    d, n, m = (self.d, self.n, self.m)
    lnÎ», Î» = self.constitute_lnÎ»_Î»(*self.lnÎ»_Î»_underlying)
    C, CÊ¹ = self.constitute_C_CÊ¹(*((lnÎ», Î») + self.C_CÊ¹_underlying))
    D, Dâ‚’ = self.constitute_D_Dâ‚’(*self.D_Dâ‚’_underlying)
    BÊ¹ = self.BÊ¹(lnÎ», Î»)

    # Î»  : [1, n]
    # kÊ¹ : [T, b, n] // [T, b, r, n]
    # sÊ¹ : [T, b, n] // [T, b, r, n]
    # y  : [T, b, m] // [T, b, m]
    kÊ¹=None
    for i in np.arange(self.Î”):
      sÊ¹ = self.run_simo_lds(EÂ·x, lnÎ», Î», BÊ¹, kÊ¹) # [T, b, r, n]
      tf.debugging.check_numerics(tf.math.real(sÊ¹), "SIMO LDS")
      if i < self.Î” - 1:
        kÊ¹ = self.corrections(sÊ¹, Î», EÂ·x)
    
    # h : [T, b, n]
    # CÊ¹ : [d, n/d, m] ?? [n, m]
    # D  : [d, m] 
    # [T, b, n, 1] * [1, 1, n, m] -> [T, b, m]
    if self.last_only:
      sÊ¹ = sÊ¹[None,-1]
      x = x[None,-1]

    if C is not None:
      # [T, b, n]
      if self.relaxV:
        h = self.direct_h_from_sÊ¹(sÊ¹, avg=True)
      else: 
        h = self.h_from_s(self.s_from_sÊ¹(sÊ¹, Î»), avg=True)
      C = tf.reshape(C, (n, m))
      # [T,b,1,n].[1,1,n,m] -> [T,b,m]
      CÂ·h = tf.matmul(h[...,None,:], C[None,None,...])[...,0,:]  
    else:
      sÊ¹ = tf.reduce_mean(sÊ¹, axis=-2)
      CÊ¹ = tf.reshape(CÊ¹, (n, m))
      # [T,b,1,n].[1,1,n,m] -> [T,b,m]
      CÂ·h = tf.math.real(tf.matmul(sÊ¹[...,None,:], CÊ¹[None,None,...])[...,0,:]) 

    DÂ·x = tf.tensordot(x, D, [[-1], [0]])
    y = CÂ·h + DÂ·x + Dâ‚’

    # Return to batch-major shape
    return y[0] if self.last_only else tf.transpose(y, (1,0,2))

  # x : [T, b, r]
  # Î» : [1, n]
  # kÊ¹ : [T, b, r, n] 
  def run_simo_lds(self, x, lnÎ», Î», BÊ¹, kÊ¹=None):
    r, n, m = (self.r, self.n, self.m)
    T,b,_ = x.shape

    # x_kÊ¹ : [T, b, r*n] 
    x_kÊ¹ = tf.repeat(x, n, -1)
    if kÊ¹ is not None:
      x_kÊ¹ += tf.reshape(kÊ¹, (T,b,r*n))   

    # TODO: replace tiling with more efficient op
    # Î» : [T, b, r*n]
    Î» = tf.tile(tf.reshape(Î», (1,1,-1)), (T, b, r)) 
    
    # sÊ¹ : [T, b, r*n] to [T, b, r, n]
    sÊ¹ = linear_recurrence(Î», x_kÊ¹)
    sÊ¹ = tf.reshape(sÊ¹, (T,b,r,n))
    return sÊ¹

class LuenbergerStack(LDStack):
  def __init__(self, n, m, Î”, AB_param, C_param, D_param, 
               E_init=tf.keras.initializers.GlorotNormal(), 
               ğ’¯_init=tf.keras.initializers.Identity(), 
               mode='standard', 
               Ï=tf.keras.activations.elu, 
               last_only=False):
    super(LuenbergerStack, self).__init__(n, m, Î”, AB_param, C_param, D_param, ğ’¯_init, Ï, mode, last_only)
    self.E_init = E_init
    
  def build(self, input_shape):
    super().build(input_shape)
    n_d = int(self.n/self.d)
    if self.n % self.d != 0:
      raise "n (LDS state size) must be divisible by d (input dimension)"
    self.lnÎ»_Î»_underlying, self.constitute_lnÎ»_Î» = self.AB_param(n_d, self.m, self.d)
    if self.d > 1:
      self.E = tf.Variable(self.E_init(shape=(self.d,self.d)), name="E", dtype=tf.float32)
    else:
      self.E = tf.ones((1,1), dtype=tf.float32)  

# Reciprocal square root nonlinearity
def recipsq(a):
#  return tf.complex(tf.math.rsqrt(1 + tf.math.square(tf.abs(a))), 0.0)
  return tf.math.rsqrt(1 + a*tf.math.conj(a))

# For ğ’¯_init
def random_controllability_matrix(shape):
  n = shape[0]
  A = np.random.normal(size=(n,n))
  B = np.random.normal(size=(n,1))
  return controllability_matrix((A,B))[0]
