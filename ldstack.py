import numpy as np
import tensorflow as tf
from linear_recurrent_net.linear_recurrent_net.tensorflow_binding import linear_recurrence, sparse_linear_recurrence

# Takes [batch size, T, d] real
# Returns [batch_size, T, m] real
# Note: requires a fixed batch size. This means you must:
# 1. Use an InputLayer in the Keras model specification. 
# 2. Pass a TF dataset which has been batched with drop_remainder=True to model.fit(), 
#    not a NumPy array. (Otherwise the last batch will be the wrong size.)
class LDStack(tf.keras.layers.Layer):
  # n: state size
  # d: input size
  # m: output size
  # k: number of random projections
  # Î”: depth of stack
  # standard: whether to compute 
  # last_only: return just the last element of the output sequence
  def __init__(self, n, d, m, k, Î”, init='unitary', standard=True, last_only=False, num_stacks=1):
    super(LDStack, self).__init__()

    # FIXME
    if d > 1 or k > 1:
      self.R = tf.Variable(np.random.normal(size=(d,k)), name="R", dtype=tf.float32, trainable=False)
    else:
      self.R = None

    self.mid_layers = []
    # Performance optimization for sparse, purely-linear case 
    if Î” == 1 and num_stacks == 1 and last_only:
      self.last_layer = SparseLDS(n, m, k, init, standard)
    else:
      for i in np.arange(num_stacks-1):
        self.mid_layers.append( LDStackInner(n, k, k, Î”, init, standard) )
      self.last_layer = LDStackInner(n, m, k, Î”, init, standard)
    self.last_only = last_only

  def call(self, x):
    if self.R is not None:
      x = tf.tensordot(x, self.R, [[-1], [0]])

    x = tf.complex(tf.transpose(x, (1,0,2)), 0.0)
    for layer in self.mid_layers:
      x = layer(x)
    y = tf.math.real(self.last_layer(x))

    if self.last_only:
      # check if last output has already been selected
      degree = len(y.shape)
      return y[-1,:] if degree > 2 else y
    else:
      return tf.transpose(y, (1,0,2))

# Average of k SIMO LDS, only returning last state. Uses much more memory-efficient SparseLinearRecurrence op
class SparseLDS(tf.keras.layers.Layer):
  def __init__(self, n, m, k, init, standard=True):
    super(SparseLDS, self).__init__()
    self.lnÎ», self.C, self.D, self.Dâ‚’ = init(n, m, k)
    self.k = k
    self.n = n
    self.m = m 
    self.standard = standard

  def call(self, x):
    T, b, k = x.shape
    n, lnÎ» = (self.n, self.lnÎ»)
    BÊ¹, CÊ¹ = BÊ¹_and_CÊ¹(lnÎ», self.C)
    Î» = tf.exp(lnÎ»)
    # linear_recurrence computes sÊ¹_t = Î»Â·sÊ¹_{t-1} + Bx_t
    # for standard LDS, we need to shift x
    # sÊ¹_t = Î»Â·sÊ¹_{t-1} + Bx_{t-1}
    if self.standard:
      x = tf.concat([tf.zeros((1,b,k), dtype=tf.complex64),  x[:-1]], axis=0)
    # [b,k,n]
    sâ‚œÊ¹ = sparse_linear_recurrence(Î», x, BÊ¹) 
    CÊ¹Â·sâ‚œÊ¹ = tf.reduce_sum(tf.expand_dims(sâ‚œÊ¹, -1)*tf.expand_dims(CÊ¹,0), axis=-2) # [b, k, m]
    DÂ·xâ‚œ = tf.expand_dims(x[-1], -1) * tf.complex(tf.expand_dims(self.D, 0), 0.0)
    yâ‚œ = tf.reduce_mean(CÊ¹Â·sâ‚œÊ¹ + DÂ·xâ‚œ, axis=1) + tf.complex(self.Dâ‚’, 0.0)
    return yâ‚œ

  # TODO: (A, B, C, D) by computing elementary symmetric polynomials 
  def canonical_lds():
    return None

# Takes time-major complex sequences, which is performant, but not compatible with Keras
class LDStackInner(tf.keras.layers.Layer):
  def __init__(self, n, m, k, Î”, init, standard=True):
    super(LDStackInner, self).__init__()
    self.lnÎ», self.C, self.D, self.Dâ‚’ = init(n,m,k)
    self.k = k
    self.n = n
    self.m = m 
    self.Î” = Î”
    self.standard = standard

    #elif isinstance(init, tuple):
    #  (Î»_init, C_init, D_init, Dâ‚’_init) = init
    #  self.lnÎ», self.C, self.D, self.Dâ‚’ = self.randroot_or_fixed_initialization(m, n, k, Î»_init, C_init, D_init, Dâ‚’_init)

  # x : [T, b, k]
  # Î» : [k, n]
  # C : [k, m, n]
  # D : [k, m]
  # Î± : [T, b, k, n] is:
  #      Î±_0, ..., Î±_{T-1} (in "standard" mode)
  #      Î±_1, ..., Î±_T     (otherwise)
  # Returns (for t in [1,...,T]):
  # sÊ¹: [T, b, k, n] is:
  #     sÊ¹_t = Î±_{t-1}Â·Î»Â·sÊ¹_{t-1}  + Bx_{t-1}   (in "standard" mode)
  #     sÊ¹_t = Î±_t    Â·Î»Â·sÊ¹_{t-1}  + Bx_t       (otherwise)
  # currently with sÊ¹_0 = 0
  # y : [T, b, k, m] is:
  #     y_t  = CÊ¹sÊ¹_t + Dx_t
  # y returned only if C, D, Dâ‚’ are provided

  def batch_simo_lds(self, x, lnÎ», C=None, D=None, Dâ‚’=None, Î±=None, standard=False):
    k, n, m = (self.k, self.n, self.m)
    T, b, d = x.shape
    states_only = C is None or D is None or Dâ‚’ is None
    BÊ¹, CÊ¹ = BÊ¹_and_CÊ¹(lnÎ», C)
    Î» = tf.exp(lnÎ»)

    # BÊ¹Â·x : [T, b, k*n]   
    BÊ¹Â·x = tf.reshape(tf.expand_dims(x, -1)*BÊ¹, (T, b, k*n))
    
    # Î±Â·Î» : [T, b, k*n]
    # sÊ¹ : [T, b, k, n]  
    if Î± is None:
      Î±Â·Î» =  tf.tile(tf.reshape(Î», (1,1,-1)), (T, b, 1)) 
    else:
      Î±Â·Î» = Î±*tf.reshape(Î», (1, 1, k, n))  
      Î±Â·Î» = tf.reshape(Î±Â·Î», (T, b, k*n))
    # linear_recurrence computes sÊ¹_t = Î±_tÂ·Î»Â·sÊ¹_{t-1} + Bx_t
    # for standard LDS, we need to shift Î± and x
    # sÊ¹_t = Î±_{t-1}Â·Î»Â·sÊ¹_{t-1} + Bx_{t-1}
    if standard:
      Î±Â·Î»  = tf.concat([tf.zeros((1,b,k*n), dtype=tf.complex64),  Î±Â·Î»[:-1]], axis=0)
      BÊ¹Â·x = tf.concat([tf.zeros((1,b,k*n), dtype=tf.complex64),  BÊ¹Â·x[:-1]], axis=0)
    sÊ¹ = linear_recurrence(Î±Â·Î», BÊ¹Â·x)
    sÊ¹ = tf.reshape(sÊ¹, [T, b, k, n])
    if states_only:
      return sÊ¹
      
    CÊ¹Â·sÊ¹ = tf.reduce_sum(tf.expand_dims(sÊ¹, -1)*tf.reshape(CÊ¹, (1,1,k,n,m)), axis=-2)
    DÂ·x = tf.expand_dims(x, -1) * tf.complex(tf.reshape(D, (1,1,k,m)), 0.0)
    y = CÊ¹Â·sÊ¹ + DÂ·x + tf.complex(Dâ‚’, 0.0)

    return sÊ¹, y

  # x: [T, batch size, k] is complex (this is unusual, but matches the format of linear_recurrence.
  #       And actually, is faster for GPU computation, and should be the standard for RNNs.)
  # Returns complex output y of shape [T, batch size, m]
  def call(self, x):
    T, b, k = x.shape
    n = self.n
      
    Î» = tf.exp(self.lnÎ»)
    # Î» : [k, n]
    # C : [k, m, n]
    # Î± : [T, b, k, n]
    # sÊ¹: [T, b, k, n]
    # y : [T, b, k, m]
    Î±=None
    for i in np.arange(self.Î” - 1):
      sÊ¹ = self.batch_simo_lds(x, self.lnÎ», Î±=Î±, standard=self.standard)
      Î»Â·sÊ¹ = tf.reshape(Î», (1,1,k,n)) * sÊ¹
      Î± = recipsq(Î»Â·sÊ¹)
    _, y = self.batch_simo_lds(x, self.lnÎ», self.C, self.D, self.Dâ‚’, Î±, self.standard)
    return tf.reduce_mean(y, axis=2)    

# Initialize with uniformly random complex eigenvalues of magnitude 1.
# If Î» has polar coordinates (r, Î¸) then ln(Î») = ln(r) + Î¸i
# Also, ln(Î»*) = ln(r) - Î¸i
# Fix r=1, i.e. ln(r) = 0. (For numerical reasons, can fix r=1-ğ›¿ for some small ğ›¿) 
# Note this only requires n/2 parameters rather than n
# Should constrain -Ï€ â‰¤ Î¸ â‰¤ Ï€
def unitary_initialization(n, m, k, trainÎ»=True, ğ›¿=0.02):
  if n % 2 != 0:
    raise "n must be even"

  half_n = round(n/2)
  Î¸_init = np.random.uniform(low=-np.pi, high=np.pi, size=[k,half_n]).astype(np.float32)
  Î¸ = tf.Variable(Î¸_init, name="eig_angle", dtype=tf.float32, trainable=trainÎ»)
  lnÎ»_r = tf.zeros((k,n), dtype=tf.float32)
  lnÎ»_r = np.log(1-ğ›¿)*tf.ones((k,n), dtype=tf.float32)
    
  lnÎ»_i = tf.concat([Î¸, -Î¸], axis=1)
  lnÎ» = tf.complex(lnÎ»_r, lnÎ»_i)

  C_init = np.random.uniform(low=-0.000001, high=0.000001, size=[k,m,n]).astype(np.float32) / n**2
  C = tf.Variable(C_init, name="C", dtype=tf.float32)
  D_init = np.random.uniform(low=-0.001, high=0.001, size=[k,m]).astype(np.float32)
  D = tf.Variable(D_init, name="D", dtype=tf.float32)
  Dâ‚’_init = np.random.uniform(low=-0.0000001, high=0.0000001, size=[m]).astype(np.float32)
  Dâ‚’ = tf.Variable(Dâ‚’_init, name="D0", dtype=tf.float32)    
    
  return lnÎ», C, D, Dâ‚’

# Initialization as roots of monic polynomial with random coefficients
def randroot_or_fixed_initialization(n, m, k, Î»_init=None, C_init=None, D_init=None, Dâ‚’_init=None, stable=True):
  if Î»_init is None:
    Î»_init = np.zeros((k,n), dtype=np.complex64)
    for i in np.arange(k):
      Ainit = np.diagflat(np.ones(shape=n-1), 1).astype(np.float32)
      Ainit[-1,:] = np.random.normal(size=n) / n
      Î»_init[i] = np.linalg.eigvals(Ainit)

    if stable:
      Î»_init = Î»_init / np.maximum(1.0, np.abs(Î»_init))
  Î»_init = Î»_init.flatten()
  
  # isolated (non paired) log eigenvalue ends up real if imaginary part is either 0 or pi 
  # only optimize over real part, fix the imaginary part. this ensures eigenvalue is real, but fixes its sign
  # (which is OK, because it shouldn't approach zero anyway.)
  lnÎ»_real_init = np.log(Î»_init[np.isreal(Î»_init)] + 0j)
  lnÎ»_real_r = tf.Variable(lnÎ»_real_init.real, name="ln_eig_real_r", dtype=tf.float32)
  lnÎ»_real = tf.complex(lnÎ»_real_r, lnÎ»_real_init.imag)

  comp_pair = (Î»_init[np.iscomplex(Î»_init)])[::2] # only get one part of conjugate pair.
  ln_comp_pair = np.log(comp_pair)
  lnÎ»_comp_init_a = ln_comp_pair.real.astype(np.float32)
  lnÎ»_comp_init_b = ln_comp_pair.imag.astype(np.float32)
            
  lnÎ»_comp_a = tf.Variable(lnÎ»_comp_init_a, name="ln_eig_comp_a", dtype=tf.float32)
  lnÎ»_comp_b = tf.Variable(lnÎ»_comp_init_b, name="ln_eig_comp_b", dtype=tf.float32)
  # Keep conjugate pairs adjacent [-b_1, b_1, -b_2, b_2, ...]
  # tf.repeat() not in 1.13
  lnÎ»_comp_r = tf.keras.backend.repeat_elements(lnÎ»_comp_a, 2, axis=0)
  lnÎ»_comp_i = tf.keras.backend.repeat_elements(lnÎ»_comp_b, 2, axis=0) * np.tile([-1,1], lnÎ»_comp_init_b.shape[0])
  lnÎ»_comp = tf.complex(lnÎ»_comp_r, lnÎ»_comp_i)
  # restore original order of eigenvalues
  # TF BUG in scatter_nd() seems to mangle complex values 
  if False:
    lnÎ» = tf.scatter_nd(
              np.concatenate((np.argwhere(np.isreal(Î»_init)), np.argwhere(np.iscomplex(Î»_init)))),
              tf.concat([lnÎ»_real, lnÎ»_comp], axis=0),
              [k*n])    
  else:
    lnÎ»_r = tf.scatter_nd(
                np.concatenate((np.argwhere(np.isreal(Î»_init)), np.argwhere(np.iscomplex(Î»_init)))),
                tf.math.real(tf.concat([lnÎ»_real, lnÎ»_comp], axis=0)),
                [k*n]) 
    lnÎ»_i = tf.scatter_nd(
                np.concatenate((np.argwhere(np.isreal(Î»_init)), np.argwhere(np.iscomplex(Î»_init)))),
                tf.math.imag(tf.concat([lnÎ»_real, lnÎ»_comp], axis=0)),
                [k*n]) 
    lnÎ» = tf.complex(lnÎ»_r, lnÎ»_i)
  lnÎ» = tf.reshape(lnÎ», [k,n])

  if C_init is None:
    C_init = np.random.uniform(low=-0.000001, high=0.000001, size=[k,m,n]).astype(np.float32) / n**2
  C = tf.Variable(C_init, name="C", dtype=tf.float32)
  if D_init is None:
    D_init = np.random.uniform(low=-0.001, high=0.001, size=[k,m]).astype(np.float32) / m
  D = tf.Variable(D_init, name="D", dtype=tf.float32)
  if Dâ‚’_init is None:
    Dâ‚’_init = 0.0
  Dâ‚’ = tf.Variable(Dâ‚’_init, name="D0", dtype=tf.float32)  

  return lnÎ», C, D, Dâ‚’

# Careful computation of numerically unstable quantities
def BÊ¹_and_CÊ¹(lnÎ», C):
  k, n = lnÎ».shape
  Î» = tf.exp(lnÎ»)
  
  # ratios[k,i,j] = Î»i / Î»j for k'th Î»
  ratios = tf.reshape(Î», (k, -1, 1)) / tf.reshape(Î», (k, 1, -1))
  #ratios = tf.exp(tf.reshape(lnÎ», (k, -1, 1)) - tf.reshape(lnÎ», (k, 1, -1)))
  ratios = tf.linalg.set_diag(ratios, tf.zeros(shape=[k,n], dtype=tf.complex64))

  # BÊ¹_i = Î»i^{n-1} / âˆ_{iâ‰ j} Î»i-Î»j
  # log BÊ¹_i 
  # = (n-1) logÎ»i - âˆ‘_{iâ‰ j} log(Î»i-Î»j)
  # = (n-1) logÎ»i - âˆ‘_{iâ‰ j} logÎ»i + log(1 - Î»j/Î»i)
  # = -âˆ‘_{iâ‰ j} log(1 - Î»j/Î»i)
  # = -âˆ‘_j log(1 - ratios[k,j,i]) 
  # because log(1 - ratios[k,i,i]) = 0
  lnBÊ¹ = -1* tf.reduce_sum(tf.math.log(1.0 - ratios), axis=1)
  # BÊ¹ : [k, n]
  BÊ¹ = tf.exp(lnBÊ¹)
  if C is None:
    return BÊ¹, None

  # CÊ¹ = CU
  Î»â± = tf.expand_dims(tf.math.pow(tf.expand_dims(Î», -1), tf.cast(tf.range(1,n+1), dtype=tf.complex64)), 0) # [1, k, n, n]
  C = tf.expand_dims(tf.transpose(tf.cast(C, tf.complex64), (1,0,2)), 2) # [m, k, 1, n]
  CÂ·Î»â± = tf.reduce_sum(C*Î»â±, axis=-1) # [m, k, n]
  CÊ¹ = tf.exp(tf.math.log(tf.cast(-n, tf.complex64)*tf.expand_dims(lnÎ», 0)) + tf.math.log(CÂ·Î»â±))
  CÊ¹ = tf.transpose(CÊ¹, (1,2,0)) # [k, n, m]
  tf.debugging.check_numerics(tf.math.real(CÊ¹), message="C'")

  return BÊ¹, CÊ¹

# Reciprocal square root nonlinearity
def recipsq(a):
  return tf.complex(tf.math.rsqrt(1 + tf.math.square(tf.abs(a))), 0.0)