import numpy as np
import tensorflow as tf
from linear_recurrent_net.linear_recurrent_net.tensorflow_binding import linear_recurrence, sparse_linear_recurrence

'''
LDStack is somewhat unusual in that it supports multiple different underlying parameterizations of the 
optimization variables. A simple example is that we can optimize over C as an n-dimensional real vector,
or over C' = CU, an n-dimensional complex vector, or over log C'. These different parameterizations must be 
initialized in different manners. They "reconstitute" the actual quantities used by the model in different
ways, as well. 

Each parameterization is defined by two functions. 
1. An initialization function, which takes options for the parameterization, and 
returns a function which is called once the dimensions of the variables are known. 
That function, in turn, returns the underlying optimization variables, as well as
the reconstitution function, defined next.
Example:
   def unitary_eig_params(trainÎ»=True, useD=True):
     def params(n,m,k):
       ...
       return underlying, Î¸
     return params

2. A reconstitution function, which takes the underlying variables and produces the quantities used by 
the model. Example:
   def unitary_eig_constitute(Î¸):
     ...
     return lnÎ», Î»
'''

# Initialize with uniformly random complex eigenvalues of magnitude 1.
# If Î» has polar coordinates (r, Î¸) then ln(Î») = ln(r) + Î¸i
# Also, ln(Î»*) = ln(r) - Î¸i
# Fix r=1, i.e. ln(r) = 0. (For numerical reasons, can fix r=1-ð›¿ for some small ð›¿) 
# Note this only requires n/2 parameters rather than n
# Should constrain -Ï€ â‰¤ Î¸ â‰¤ Ï€
def unitary_AB_param(trainÎ»=True):
  def params(n, m, k):
    if n % 2 != 0:
      raise "n must be even"
    half_n = round(n/2)
    Î¸_init = np.random.uniform(low=-np.pi, high=np.pi, size=[k,half_n]).astype(np.float32)
    Î¸ = tf.Variable(Î¸_init, name="eig_angle", dtype=tf.float32, trainable=trainÎ»)
    return (Î¸,), unitary_AB_constitute
  return params

def unitary_AB_constitute(Î¸):
  k, half_n = Î¸.shape
  lnÎ»_r = tf.zeros((k,half_n*2), dtype=tf.float32)
  lnÎ»_i = tf.concat([Î¸, -Î¸], axis=1)
  lnÎ» = tf.complex(lnÎ»_r, lnÎ»_i)
  Î» = tf.exp(lnÎ»)
  return lnÎ», Î»

# Initialization as roots of monic polynomial with random coefficients
def randroot_AB_param(max_radius=1.0, standard=False):
  def params(n,m,k):
    Î»_init = np.zeros((k,n), dtype=np.complex64)
    for i in np.arange(k):
      Ainit = np.diagflat(np.ones(shape=n-1), 1).astype(np.float32)
      Ainit[-1,:] = np.random.normal(size=n) / n
      Î»_init[i] = np.linalg.eigvals(Ainit)

    if max_radius is not None:
      Î»_init = Î»_init / np.maximum(max_radius, np.abs(Î»_init))
    return standard_AB_param(Î»_init)(n,m,k) if standard else log_AB_param(Î»_init)(n,m,k)
  return params


def log_AB_param(Î»_init):
  Î»_init_val = Î»_init # Python local variable strangeness
  def params(n,m,k):
    Î»_init = Î»_init_val.flatten()
    
    # isolated (unpaired) log eigenvalue ends up real if imaginary part is either 0 or pi 
    # only optimize over real part, fix the imaginary part. this ensures eigenvalue is real, but fixes its sign
    # (which is OK, because it shouldn't approach zero anyway.)
    lnÎ»_real_init = np.log(Î»_init[np.isreal(Î»_init)] + 0j)
    lnÎ»_real_r = tf.Variable(lnÎ»_real_init.real, name="ln_eig_real_r", dtype=tf.float32)

    comp_pair = (Î»_init[np.iscomplex(Î»_init)])[::2] # only get one part of conjugate pair.
    ln_comp_pair = np.log(comp_pair)
    lnÎ»_comp_init_a = ln_comp_pair.real.astype(np.float32)
    lnÎ»_comp_init_b = ln_comp_pair.imag.astype(np.float32)
              
    lnÎ»_comp_a = tf.Variable(lnÎ»_comp_init_a, name="ln_eig_comp_a", dtype=tf.float32)
    lnÎ»_comp_b = tf.Variable(lnÎ»_comp_init_b, name="ln_eig_comp_b", dtype=tf.float32)
    
    where_Î»_init_r = np.argwhere(np.isreal(Î»_init))
    where_Î»_init_i = np.argwhere(np.iscomplex(Î»_init))

    return (lnÎ»_real_r, lnÎ»_real_init.imag, lnÎ»_comp_a, lnÎ»_comp_b, where_Î»_init_r, where_Î»_init_i, k, n), log_AB_constitute
  return params

def log_AB_constitute(lnÎ»_real_r, lnÎ»_real_i_init, lnÎ»_comp_a, lnÎ»_comp_b, where_Î»_init_r, where_Î»_init_i, k, n):
  lnÎ»_real = tf.complex(lnÎ»_real_r, lnÎ»_real_i_init)

  # Keep conjugate pairs adjacent [-b_1, b_1, -b_2, b_2, ...]
  lnÎ»_comp_r = tf.repeat(lnÎ»_comp_a, 2, axis=0)
  lnÎ»_comp_i = tf.repeat(lnÎ»_comp_b, 2, axis=0) * np.tile([-1,1], lnÎ»_comp_b.shape[0])
  lnÎ»_comp = tf.complex(lnÎ»_comp_r, lnÎ»_comp_i)
  # restore original order of eigenvalues
  lnÎ» = tf.scatter_nd(
              np.concatenate((where_Î»_init_r, where_Î»_init_i)),
              tf.concat([lnÎ»_real, lnÎ»_comp], axis=0),
              [k*n])    
  lnÎ» = tf.reshape(lnÎ», [k,n])
  Î» = tf.exp(lnÎ»)

  return lnÎ», Î»

def standard_AB_param(Î»_init):
  Î»_init_val = Î»_init # Python local variable strangeness
  def params(n,m,k):
    Î»_init = Î»_init_val.flatten()
    
    Î»_real_init = Î»_init[np.isreal(Î»_init)]
    Î»_real_r = tf.Variable(Î»_real_init.real, name="eig_real_r", dtype=tf.float32)

    comp_pair = (Î»_init[np.iscomplex(Î»_init)])[::2] # only get one part of conjugate pair.
    Î»_comp_init_a = comp_pair.real.astype(np.float32)
    Î»_comp_init_b = comp_pair.imag.astype(np.float32)
              
    Î»_comp_a = tf.Variable(Î»_comp_init_a, name="eig_comp_a", dtype=tf.float32)
    Î»_comp_b = tf.Variable(Î»_comp_init_b, name="eig_comp_b", dtype=tf.float32)
    
    where_Î»_init_r = np.argwhere(np.isreal(Î»_init))
    where_Î»_init_i = np.argwhere(np.iscomplex(Î»_init))

    return (Î»_real_r, Î»_comp_a, Î»_comp_b, where_Î»_init_r, where_Î»_init_i, k, n), standard_AB_constitute
  return params

def standard_AB_constitute(Î»_real_r, Î»_comp_a, Î»_comp_b, where_Î»_init_r, where_Î»_init_i, k, n):
  Î»_real = tf.complex(Î»_real_r, 0.0)

  # Keep conjugate pairs adjacent [-b_1, b_1, -b_2, b_2, ...]
  Î»_comp_r = tf.repeat(Î»_comp_a, 2, axis=0)
  Î»_comp_i = tf.repeat(Î»_comp_b, 2, axis=0) * np.tile([-1,1], Î»_comp_b.shape[0])
  Î»_comp = tf.complex(Î»_comp_r, Î»_comp_i)
  # restore original order of eigenvalues
  Î» = tf.scatter_nd(
              np.concatenate((where_Î»_init_r, where_Î»_init_i)),
              tf.concat([Î»_real, Î»_comp], axis=0),
              [k*n])    
  Î» = tf.reshape(Î», [k,n])
  lnÎ» = tf.math.log(Î»)

  return lnÎ», Î»

def canonical_AB_param(a_stddev = 0.00001):
  def params(n,m,k):
    a = tf.Variable(tf.random.normal((k,n), stddev=a_stddev) / float(n), name="a", dtype=tf.float32)
    return (a,), canonical_AB_constitute
  return params

def canonical_AB_constitute(a):
  k,n = a.shape
  A = np.diagflat(np.ones(shape=n-1), 1)[:-1]
  A = np.tile(A.reshape(1,n-1,n), (k,1,1))
  A = A.astype(np.float32)
  a_ = tf.expand_dims(a, 1)
  A = tf.concat([A, -a_], axis=1)
  Î» = tf.linalg.eigvals(A) #FIXME: double work?
  lnÎ» = tf.math.log(Î»)
  return lnÎ», Î»

# Optimize over real C, potentially encountering numerical stability
def standard_C_param(C_init=None, C_stddev=0.00001):
  def params(n,m,k):
    C = tf.Variable(tf.random.normal((k,n,m), stddev=C_stddev), name="C", dtype=tf.float32)
    return (C,), standard_C_constitute
  return params

def standard_C_constitute(lnÎ», Î», C):
  CÊ¹ = computeCÊ¹(lnÎ», Î», C) 
  return CÊ¹


# Expands optimization over real C to complex CÊ¹.
# After optimization, CU â‰ˆ CÊ¹ with equivalent loss can be found 
# Furthermore, optimize over lnCÊ¹ for numerical reasons
# (typically, C is very close to 0)
# Uses lnÎ», Î» for initial value

def relaxed_C_param(CÊ¹_mean=0.0, CÊ¹_stddev=1):
  def params(n,m,k):
    CÊ¹_r = tf.Variable(tf.random.normal((k,n,m), mean=CÊ¹_mean, stddev=CÊ¹_stddev), name="C'_r", dtype=tf.float32)
    CÊ¹_i = tf.Variable(tf.random.normal((k,n,m), mean=CÊ¹_mean, stddev=CÊ¹_stddev), name="C'_i", dtype=tf.float32)
    return (CÊ¹_r, CÊ¹_i), relaxed_C_constitute
  return params

def relaxed_C_constitute(lnÎ», Î», CÊ¹_r, CÊ¹_i):
  CÊ¹ = tf.complex(CÊ¹_r, CÊ¹_i)
  return CÊ¹

def relaxed_log_C_param(CÊ¹_mean=1.0, CÊ¹_stddev=0.01):
  def params(n,m,k):
    lnCÊ¹_init = tf.math.log(tf.cast(tf.random.normal((k,n,m), mean=CÊ¹_mean, stddev=CÊ¹_stddev), tf.complex64))
    lnCÊ¹_r = tf.Variable(tf.math.real(lnCÊ¹_init), name="lnC'_r", dtype=tf.float32)
    lnCÊ¹_i = tf.Variable(tf.math.imag(lnCÊ¹_init), name="lnC'_i", dtype=tf.float32)
    return (lnCÊ¹_r, lnCÊ¹_i), relaxed_log_C_constitute
  return params

def relaxed_log_C_constitute(lnÎ», Î», lnCÊ¹_r, lnCÊ¹_i):
  lnCÊ¹ = tf.complex(lnCÊ¹_r, lnCÊ¹_i)
  CÊ¹ = tf.exp(lnCÊ¹)
  return CÊ¹

def reciproot_C_param(init_stddev=100, jitter=5):
  def params(n,m,k):
    #slide = tf.reshape(tf.math.pow(init_base, tf.cast(tf.range(n, 0, delta=-1), tf.float32)), (1,n,1))
    #C_init = tf.random.normal((k,n,m), stddev=init_stddev)*slide
    #print(C_init[0,:,0], "C init")
    #C_initâ‚š =  C_init*tf.cast(C_init > 0, tf.float32) + tf.random.uniform((k,n,m), minval=0, maxval=jitter)
    #C_initâ‚™ = -C_init*tf.cast(C_init < 0, tf.float32) + tf.random.uniform((k,n,m), minval=0, maxval=jitter)
    #p = tf.reshape(tf.math.reciprocal(tf.cast(tf.range(1,n) - n, tf.float32)), (1,n-1,1)) 
    #Ïˆâ‚š_init = tf.math.pow(C_initâ‚š[:,:-1], p)
    #print(Ïˆâ‚š_init[0,:,0], "init")
    #Ïˆâ‚™_init = tf.math.pow(C_initâ‚™[:,:-1], p)  
    slide = tf.reshape(tf.math.reciprocal(tf.square(tf.cast(tf.range(n, 0, delta=-1), tf.float32))), (1,n,1))
    C_init = tf.cast(tf.random.normal((k,n,m), stddev=init_stddev)*slide, tf.complex64)
    p = tf.reshape(tf.math.reciprocal(tf.cast(tf.range(1,n) - n, tf.complex64)), (1,n-1,1)) 
    Ïˆ_init = tf.math.pow(C_init[:,:-1], p)
    Ïˆâ‚š_init =  Ïˆ_init*tf.cast(Ïˆ_init > 0, tf.float32) + tf.random.uniform((k,n-1,m), minval=0, maxval=jitter)
    Ïˆâ‚™_init = -Ïˆ_init*tf.cast(Ïˆ_init < 0, tf.float32) + tf.random.uniform((k,n-1,m), minval=0, maxval=jitter)
    Câ‚šâ‚™_init = tf.random.normal((k,1,m))
    Câ‚™â‚™_init = tf.random.normal((k,1,m))
    tf.debugging.check_numerics(Ïˆâ‚š_init, message="Ïˆâ‚š_init")
    tf.debugging.check_numerics(Ïˆâ‚™_init, message="Ïˆâ‚™_init")
    Ïˆâ‚š = tf.Variable(Ïˆâ‚š_init, name="Ïˆp", constraint=tf.keras.constraints.NonNeg())
    Câ‚šâ‚™ = tf.Variable(Câ‚šâ‚™_init, name="Cpn", constraint=tf.keras.constraints.NonNeg())
    Ïˆâ‚™ = tf.Variable(Ïˆâ‚™_init, name="Ïˆn", constraint=tf.keras.constraints.NonNeg())
    Câ‚™â‚™ = tf.Variable(Câ‚™â‚™_init, name="Cnn", constraint=tf.keras.constraints.NonNeg())
    return (Ïˆâ‚š,Câ‚šâ‚™,Ïˆâ‚™,Câ‚™â‚™), reciproot_C_constitute 
  return params

def reciproot_C_constitute(lnÎ», Î», Ïˆâ‚š,Câ‚šâ‚™,Ïˆâ‚™,Câ‚™â‚™):
  #mâ‚š = tf.logical_and(tf.math.is_finite(Ïˆ), Ïˆ >= 0.0) 
  #mâ‚™ = tf.logical_and(tf.math.is_finite(Ïˆ), Ïˆ < 0.0)
  CÊ¹ = Ïƒ(Î», Ïˆâ‚š, Câ‚šâ‚™) - Ïƒ(Î», Ïˆâ‚™, Câ‚™â‚™)
  tf.debugging.check_numerics(tf.math.real(CÊ¹), message="CÊ¹")
  return CÊ¹

def relaxed_reciproot_C_param(init_gap = 2, init_stddev=0.1):
  def params(n,m,k):
    slide = tf.reshape(tf.math.reciprocal(tf.square(tf.cast(tf.range(n, 0, delta=-1), tf.float32))), (1,n,1))
    C_init = tf.cast(tf.random.normal((k,n,m), stddev=init_stddev)*slide, tf.complex64)
    p = tf.reshape(tf.math.reciprocal(tf.cast(tf.range(1,n) - n, tf.complex64)), (1,n-1,1)) 
    Ïˆ_init = tf.math.pow(C_init[:,:-1], p)
    #print(Ïˆ_init[0,:,0])

    Ïˆ_init_r = tf.reshape(tf.range(n-1, 0, -1, tf.float32), (1,n-1,1)) + tf.random.uniform((k,n-1,m), minval=-0.1, maxval=0.1)
    Ïˆ_init_i = tf.random.uniform((k,n-1,m), minval=-0.1, maxval=0.1)    

    Ïˆ_r = tf.Variable(Ïˆ_init_r, name="Ïˆ_r")
    Ïˆ_i = tf.Variable(Ïˆ_init_i, name="Ïˆ_i")
    Câ‚™ = tf.Variable(tf.random.normal((k,1,m)), name="C_n")
    return (Ïˆ_r,Ïˆ_i,Câ‚™), relaxed_reciproot_C_constitute 
  return params    

def relaxed_reciproot_C_constitute(lnÎ», Î», Ïˆ_r, Ïˆ_i, Câ‚™):
  Ïˆ = tf.complex(Ïˆ_r, Ïˆ_i)   
  CÊ¹ = Ïƒ(Î», Ïˆ, Câ‚™) 
  #tf.debugging.check_numerics(tf.math.real(CÊ¹), message="CÊ¹")
  return CÊ¹

# Ïˆ : [k,n,m] is one of the halves
# Câ‚™ : [k,1,m]
def Ïƒ(Î», Ïˆ, Câ‚™):
  k,n = Î».shape
  Ïˆ_ = tf.expand_dims(tf.cast(Ïˆ, tf.complex64), -2) # [k, n-1, 1, m]  
  #m_ = tf.expand_dims(tf.cast(m, tf.complex64), -2) # [k, n, 1, m]
  Î»_ = tf.reshape(Î», (k,1,n,1))
  ÏˆÂ·Î» = Ïˆ_ * Î»_ # [k, n-1, n, m]. dim 1 indexed by p, 2 by j
  p = tf.reshape(tf.cast(tf.range(1,n) - n, dtype=tf.complex64), (1,n-1,1,1))
  ÏˆÂ·Î»â± = tf.math.pow(ÏˆÂ·Î», p) 
  #tf.debugging.check_numerics(tf.math.real(ÏˆÂ·Î»â±), message="ÏˆÂ·Î»â±")
  return tf.reduce_sum(ÏˆÂ·Î»â±, axis=1) + tf.cast(Câ‚™, tf.complex64)

def standard_D_param(D_init=None, Dâ‚’_init=None, useD=True):
  def params(n,m,k):
    if useD:
      D = tf.Variable(
          np.random.uniform(low=-0.001, high=0.001, size=[k,m]).astype(np.float32) / m if D_init is None else D_init,
          name="D", dtype=tf.float32)
      Dâ‚’ = tf.Variable(
          0.0 if Dâ‚’_init is None else Dâ‚’_init, 
          name="D0", dtype=tf.float32)  
    else:
      D = tf.zeros((k,m), dtype=tf.float32)
      Dâ‚’ = 0.0
    return (D, Dâ‚’), lambda D, Dâ‚’: (D,Dâ‚’)
  return params


# Takes [batch size, T, d] real
# Returns [batch_size, T, m] real (in averaging mode)
#      or [batch_size, T, d, m]   (with k=None)
# Note: requires a fixed batch size. This means you must:
# 1. Use an InputLayer in the Keras model specification. 
# 2. Pass a TF dataset which has been batched with drop_remainder=True to model.fit(), 
#    not a NumPy array. (Otherwise the last batch will be the wrong size.)
class LDStack(tf.keras.layers.Layer):
  # n: state size
  # d: input size
  # m: output size
  # k: number of random projections (averaging mode)
  # Î”: depth of stack
  # standard: whether to compute 
  # last_only: return just the last element of the output sequence
  def __init__(self, n, d, m, k=None, Î”=1, 
               AB_param=randroot_AB_param(), 
               C_param=reciproot_C_param(), 
               D_param=standard_D_param(), 
               standard=True, last_only=False, num_stacks=1):
    super(LDStack, self).__init__()
    self.n = n
    self.m = m
    self.k = k
    self.Î” = Î”
    self.AB_param = AB_param
    self.C_param = C_param
    self.D_param = D_param
    self.standard = standard
    self.average = k is not None
    self.last_only = last_only
    self.num_stacks = num_stacks

  def build(self, input_shape):
    self.b, self.T, d = input_shape
    n, m, k, Î”, standard, average, last_only, num_stacks = (self.n, self.m, self.k, self.Î”, self.standard, self.average, self.last_only, self.num_stacks) 

    # Only perform random projection if number of projections is specified
    # Otherwise, run SIMO LDS on each coordinate of original input
    if average:
      self.R = tf.Variable(np.random.normal(size=(d,k)), name="R", dtype=tf.float32, trainable=False)
    else:
      k = d

    self.mid_layers = []
    # Performance optimization for sparse, purely-linear case 
    if Î” == 1 and num_stacks == 1 and last_only:
      self.last_layer = SparseLDS(n, m, k, self.AB_param, self.C_param, self.D_param, average, standard)
    else:
      for i in np.arange(num_stacks-1):
        self.mid_layers.append( LDStackInner(n, k, k, Î”, self.AB_param, self.C_param, self.D_param, average, standard) )
      self.last_layer = LDStackInner(n, m, k, Î”, self.AB_param, self.C_param, self.D_param, average, standard, last_only=last_only)

  def call(self, x):
    if self.average:
      x = tf.tensordot(x, self.R, [[-1], [0]])

    x = tf.complex(tf.transpose(x, (1,0,2)), 0.0)
    for layer in self.mid_layers:
      x = layer(x)
      x = tf.reshape(x, (self.T, self.b, -1))
    y = tf.math.real(self.last_layer(x))

    # Return to batch-major shape 
    if self.last_only:
      # time axis already eliminated when last state was taken
      return y
    else:
      degree = len(y.shape)
      return tf.transpose(y, (1,0,2,3)) if degree == 4 else tf.transpose(y, (1,0,2))


# Average of k SIMO LDS, only returning last state. Uses much more memory-efficient SparseLinearRecurrence op
class SparseLDS(tf.keras.layers.Layer):
  def __init__(self, n, m, k, AB_param, C_param, D_param, average, standard=True):
    super(SparseLDS, self).__init__()
    self.lnÎ»_Î»_underlying, self.constitute_lnÎ»_Î» = AB_param(n, m, k)
    self.CÊ¹_underlying, self.constitute_CÊ¹ = C_param(n, m, k)
    self.D_Dâ‚’_underlying, self.constitute_D_Dâ‚’ = D_param(n, m, k)
    self.k = k
    self.n = n
    self.m = m 
    self.average = average
    self.standard = standard

  def call(self, x):
    T, b, k = x.shape
    n = self.n
    lnÎ», Î» = self.constitute_lnÎ»_Î»(*self.lnÎ»_Î»_underlying)
    CÊ¹ = self.constitute_CÊ¹(*((lnÎ», Î») + self.CÊ¹_underlying))
    D, Dâ‚’ = self.constitute_D_Dâ‚’(*self.D_Dâ‚’_underlying)
    BÊ¹ = computeBÊ¹(lnÎ», Î»)

    # linear_recurrence computes sÊ¹_t = Î»Â·sÊ¹_{t-1} + Bx_t
    # for standard LDS, we need to shift x
    # sÊ¹_t = Î»Â·sÊ¹_{t-1} + Bx_{t-1}
    if self.standard:
      x = tf.concat([tf.zeros((1,b,k), dtype=tf.complex64),  x[:-1]], axis=0)
    # [b,k,n]
    sâ‚œÊ¹ = sparse_linear_recurrence(Î», x, BÊ¹) 
    # sâ‚œÊ¹: [b, k, n]
    # CÊ¹: [k, n, m]    
    # [b, k, n, 1] * [1, k, n, m] -> [b, k, m]
    CÊ¹Â·sâ‚œÊ¹ = tf.reduce_sum(tf.expand_dims(sâ‚œÊ¹, -1)*tf.expand_dims(CÊ¹,0), axis=-2)
    DÂ·xâ‚œ = tf.expand_dims(x[-1], -1) * tf.complex(tf.expand_dims(D, 0), 0.0)
    yâ‚œ = CÊ¹Â·sâ‚œÊ¹ + DÂ·xâ‚œ + tf.complex(Dâ‚’, 0.0)
    
    if self.average:
      yâ‚œ = tf.reduce_mean(yâ‚œ, axis=1) 
    return yâ‚œ

  # TODO: (A, B, C, D) by computing elementary symmetric polynomials 
  def canonical_lds():
    return None

# Full generality, but slower
class LDStackInner(tf.keras.layers.Layer):
  def __init__(self, n, m, k, Î”, AB_param, C_param, D_param, average, standard=True, last_only=False):
    super(LDStackInner, self).__init__()
    self.lnÎ»_Î»_underlying, self.constitute_lnÎ»_Î» = AB_param(n, m, k)
    self.CÊ¹_underlying, self.constitute_CÊ¹ = C_param(n, m, k)
    self.D_Dâ‚’_underlying, self.constitute_D_Dâ‚’ = D_param(n, m, k)
    self.n = n
    self.m = m 
    self.k = k
    self.Î” = Î”
    self.average = average
    self.standard = standard
    self.last_only = last_only

  def build(self, input_shape):
    self.T, self.b, _ = input_shape 

  # x : [T, b, k]
  # Î» : [k, n]
  # C : [k, m, n]
  # D : [k, m]
  # Î± : [T, b, k, n] is:
  #      Î±_0, ..., Î±_{T-1} (in "standard" mode)
  #      Î±_1, ..., Î±_T     (otherwise)
  # Returns (for t in [1,...,T]):
  # sÊ¹: [T, b, k, n] is:
  #     sÊ¹_t = Î±_{t-1}Â·Î»Â·sÊ¹_{t-1}  + Bx_{t-1}   (in "standard" mode)
  #     sÊ¹_t = Î±_t    Â·Î»Â·sÊ¹_{t-1}  + Bx_t       (otherwise)
  # currently with sÊ¹_0 = 0
  # y : [T, b, k, m] is:
  #     y_t  = CÊ¹sÊ¹_t + Dx_t
  # y returned only if C, D, Dâ‚’ are provided

  def batch_simo_lds(self, x, lnÎ», Î», CÊ¹=None, D=None, Dâ‚’=None, Î±=None, standard=True):
    k, n, m, T, b = (self.k, self.n, self.m, self.T, self.b)
    states_only = CÊ¹ is None or D is None or Dâ‚’ is None
    BÊ¹ = computeBÊ¹(lnÎ», Î»)

    # BÊ¹ : [k, n]  
    # BÊ¹Â·x : [T, b, k*n]   
    BÊ¹Â·x = tf.reshape(tf.expand_dims(x, -1)*BÊ¹, (T, b, k*n))
    
    # Î±Â·Î» : [T, b, k*n]
    # sÊ¹ : [T, b, k, n]  
    if Î± is None:
      Î±Â·Î» =  tf.tile(tf.reshape(Î», (1,1,-1)), (T, b, 1)) 
    else:
      Î±Â·Î» = Î±*tf.reshape(Î», (1, 1, k, n))  
      Î±Â·Î» = tf.reshape(Î±Â·Î», (T, b, k*n))
    # linear_recurrence computes sÊ¹_t = Î±_tÂ·Î»Â·sÊ¹_{t-1} + Bx_t
    # for standard LDS, we need to shift Î± and x
    # sÊ¹_t = Î±_{t-1}Â·Î»Â·sÊ¹_{t-1} + Bx_{t-1}
    if standard:
      Î±Â·Î»  = tf.concat([tf.zeros((1,b,k*n), dtype=tf.complex64),  Î±Â·Î»[:-1]], axis=0)
      BÊ¹Â·x = tf.concat([tf.zeros((1,b,k*n), dtype=tf.complex64),  BÊ¹Â·x[:-1]], axis=0)
    sÊ¹ = linear_recurrence(Î±Â·Î», BÊ¹Â·x)
    sÊ¹ = tf.reshape(sÊ¹, [T, b, k, n])
    if states_only:
      return sÊ¹
      
    # sÊ¹ : [T,b,k,n]
    # CÊ¹ : [k,n,m] 
    # [T,b,k,n,1] * [1,1,k,n,m] -> [T,b,k,m]
    CÊ¹Â·sÊ¹ = tf.reduce_sum(tf.expand_dims(sÊ¹, -1)*tf.reshape(CÊ¹, (1,1,k,n,m)), axis=-2)
    DÂ·x = tf.expand_dims(x, -1) * tf.complex(tf.reshape(D, (1,1,k,m)), 0.0)
    y = CÊ¹Â·sÊ¹ + DÂ·x + tf.complex(Dâ‚’, 0.0)

    return sÊ¹, y

  # x: [T, batch size, k] is complex (this is unusual, but matches the format of linear_recurrence.
  #       And actually, is faster for GPU computation, and should be the standard for RNNs.)
  # Returns complex output y of shape [T, batch size, m]
  def call(self, x):
    T, b, k = x.shape
    n = self.n
    lnÎ», Î» = self.constitute_lnÎ»_Î»(*self.lnÎ»_Î»_underlying)
    CÊ¹ = self.constitute_CÊ¹(*((lnÎ», Î») + self.CÊ¹_underlying))
    D, Dâ‚’ = self.constitute_D_Dâ‚’(*self.D_Dâ‚’_underlying)

    # Î» : [k, n]
    # Î± : [T, b, k, n]
    # sÊ¹: [T, b, k, n]
    # y : [T, b, k, m]
    Î±=None
    for i in np.arange(self.Î” - 1):
      sÊ¹ = self.batch_simo_lds(x, lnÎ», Î», Î±=Î±, standard=self.standard)
      Î»Â·sÊ¹ = tf.reshape(Î», (1,1,k,n)) * sÊ¹
      Î± = recipsq(Î»Â·sÊ¹)
    _, y = self.batch_simo_lds(x, lnÎ», Î», CÊ¹, D, Dâ‚’, Î±, self.standard)
    if self.average:
      y = tf.reduce_mean(y, axis=2)
    if self.last_only:
      y = y[-1]
    return y


# Careful computation of numerically unstable quantities
def computeBÊ¹(lnÎ», Î»):
  k, n = lnÎ».shape
  
  # ratios[k,i,j] = Î»i / Î»j for k'th Î»
  ratios = tf.reshape(Î», (k, -1, 1)) / tf.reshape(Î», (k, 1, -1))
  #ln_ratios = tf.reshape(lnÎ», (k, -1, 1)) - tf.reshape(lnÎ», (k, 1, -1))
  ratios = tf.linalg.set_diag(ratios, tf.zeros(shape=[k,n], dtype=tf.complex64))

  # BÊ¹_i = Î»i^{n-1} / âˆ_{iâ‰ j} Î»i-Î»j
  # log BÊ¹_i 
  # = (n-1) logÎ»i - âˆ‘_{iâ‰ j} log(Î»i-Î»j)
  # = (n-1) logÎ»i - âˆ‘_{iâ‰ j} logÎ»i + log(1 - Î»j/Î»i)
  # = -âˆ‘_{iâ‰ j} log(1 - Î»j/Î»i)
  # = -âˆ‘_j log(1 - ratios[k,j,i]) 
  # because log(1 - ratios[k,i,i]) = 0
  lnBÊ¹ = -1* tf.reduce_sum(tf.math.log1p(-ratios), axis=1)
  # BÊ¹ : [k, n]
  BÊ¹ = tf.exp(lnBÊ¹)
  return BÊ¹


# C is [k,n,m]. Returns [k,n,m]
# in split form, where CÊ¹ = Ïˆâ‚š - Ïˆâ‚˜  
def computeCÊ¹(lnÎ», Î», C, first_method=True): 
  k,n = Î».shape

  if True:
    # Naive
    p = tf.reshape(tf.cast(tf.range(1,n+1) - n, dtype=tf.complex64), (1,n,1,1))
    Î»â± = tf.exp(p * tf.reshape(lnÎ», (k,1,n,1))) # [k,n,n,1]
    tf.debugging.check_numerics(tf.math.real(Î»â±), message="Î»â±")
    C_ = tf.expand_dims(tf.cast(C, tf.complex64), -2) # [k, n, 1, m] 
    CÊ¹ = tf.reduce_sum(C_*Î»â±, axis=2) # [k, n, m] 
  elif first_method:
    # Avoids taking log(C), but does need to take log(CÂ·Î»â±)
    Î»â± = tf.expand_dims(tf.math.pow(tf.expand_dims(Î», -1), tf.cast(tf.range(1,n+1), dtype=tf.complex64)), 0) # [1, k, n, n]  
    tf.debugging.check_numerics(tf.math.real(Î»â±), message="Î»â±")
    C = tf.expand_dims(tf.transpose(tf.cast(C, tf.complex64), (2,0,1)), 2) # [m, k, 1, n] 
    CÂ·Î»â± = tf.reduce_sum(C*Î»â±, axis=-1) # [m, k, n]  
    CÊ¹ = tf.exp(tf.cast(-n, tf.complex64)*tf.expand_dims(lnÎ», 0) + tf.math.log(CÂ·Î»â±)) # [1,k,n]+[m,k,n]
    CÊ¹ = tf.transpose(CÊ¹, (1,2,0)) # [k, n, m]
  else:
    # Takes log(C)
    lnC = tf.transpose(tf.math.log(tf.complex(C, 0.0)), (2,0,1)) # [m, k, n]
    p = tf.cast(n - tf.range(1,n+1), tf.complex64)
    lnU = tf.reshape(-1*p, (1,-1,1)) * tf.expand_dims(lnÎ», 1) # [1,n,1]*[k,1,n] -> [k, n, n] 
    lnC_lnU = tf.expand_dims(lnC,-1) + tf.expand_dims(lnU, 0) # [m,k,n,1]+[1,k,n,n] -> [m, k, n, n]
    CÊ¹ = tf.reduce_sum(tf.exp(lnC_lnU), axis=-2) # [m, k, n]
    CÊ¹ = tf.transpose(CÊ¹, (1,2,0)) # [k, n, m] 
  tf.debugging.check_numerics(tf.math.real(CÊ¹), message="C'")
  return CÊ¹



# Reciprocal square root nonlinearity
def recipsq(a):
#  return tf.complex(tf.math.rsqrt(1 + tf.math.square(tf.abs(a))), 0.0)
  return tf.math.rsqrt(1 + a*tf.math.conj(a))
