import numpy as np
import tensorflow as tf
from linear_recurrent_net.linear_recurrent_net.tensorflow_binding import linear_recurrence, sparse_linear_recurrence

'''
LDStack is somewhat unusual in that it supports multiple different underlying parameterizations of the 
optimization variables. A simple example is that we can optimize over C as an n-dimensional real vector,
or over C' = CU, an n-dimensional complex vector, or over log C'. These different parameterizations must be 
initialized in different manners. They "reconstitute" the actual quantities used by the model in different
ways, as well. 

Each parameterization is defined by two functions. 
1. An initialization function, which takes options for the parameterization, and 
returns a function which is called once the dimensions of the variables are known. 
That function, in turn, returns the underlying optimization variables, as well as
the reconstitution function, defined next.
Example:
   def unitary_eig_params(trainÎ»=True, useD=True):
     def params(n,m,k):
       ...
       return underlying, Î¸
     return params

2. A reconstitution function, which takes the underlying variables and produces the quantities used by 
the model. Example:
   def unitary_eig_constitute(Î¸):
     ...
     return lnÎ», Î»
'''

# Initialize with uniformly random complex eigenvalues of magnitude 1.
# If Î» has polar coordinates (r, Î¸) then ln(Î») = ln(r) + Î¸i
# Also, ln(Î»*) = ln(r) - Î¸i
# Fix r=1, i.e. ln(r) = 0. (For numerical reasons, can fix r=1-ð›¿ for some small ð›¿) 
# Note this only requires n/2 parameters rather than n
# Should constrain -Ï€ â‰¤ Î¸ â‰¤ Ï€
def unitary_eig_param(trainÎ»=True):
  def params(n, m, k):
    if n % 2 != 0:
      raise "n must be even"
    half_n = round(n/2)
    Î¸_init = np.random.uniform(low=-np.pi, high=np.pi, size=[k,half_n]).astype(np.float32)
    Î¸ = tf.Variable(Î¸_init, name="eig_angle", dtype=tf.float32, trainable=trainÎ»)
    return (Î¸,), unitary_eig_constitute
  return params

def unitary_eig_constitute(Î¸):
  k, half_n = Î¸.shape
  lnÎ»_r = tf.zeros((k,half_n*2), dtype=tf.float32)
  lnÎ»_i = tf.concat([Î¸, -Î¸], axis=1)
  lnÎ» = tf.complex(lnÎ»_r, lnÎ»_i)
  Î» = tf.exp(lnÎ»)
  return lnÎ», Î»

# Initialization as roots of monic polynomial with random coefficients
def randroot_eig_param(stable=True):
  def params(n,m,k):
    Î»_init = np.zeros((k,n), dtype=np.complex64)
    for i in np.arange(k):
      Ainit = np.diagflat(np.ones(shape=n-1), 1).astype(np.float32)
      Ainit[-1,:] = np.random.normal(size=n) / n
      Î»_init[i] = np.linalg.eigvals(Ainit)

    if stable:
      Î»_init = Î»_init / np.maximum(1.0, np.abs(Î»_init))

    return log_eig_param(Î»_init)
  return params


def log_eig_param(Î»_init):
  def params(n,m,k):
    Î»_init = Î»_init.flatten()
    
    # isolated (unpaired) log eigenvalue ends up real if imaginary part is either 0 or pi 
    # only optimize over real part, fix the imaginary part. this ensures eigenvalue is real, but fixes its sign
    # (which is OK, because it shouldn't approach zero anyway.)
    lnÎ»_real_init = np.log(Î»_init[np.isreal(Î»_init)] + 0j)
    lnÎ»_real_r = tf.Variable(lnÎ»_real_init.real, name="ln_eig_real_r", dtype=tf.float32)

    comp_pair = (Î»_init[np.iscomplex(Î»_init)])[::2] # only get one part of conjugate pair.
    ln_comp_pair = np.log(comp_pair)
    lnÎ»_comp_init_a = ln_comp_pair.real.astype(np.float32)
    lnÎ»_comp_init_b = ln_comp_pair.imag.astype(np.float32)
              
    lnÎ»_comp_a = tf.Variable(lnÎ»_comp_init_a, name="ln_eig_comp_a", dtype=tf.float32)
    lnÎ»_comp_b = tf.Variable(lnÎ»_comp_init_b, name="ln_eig_comp_b", dtype=tf.float32)
    
    where_Î»_init_r = np.argwhere(np.isreal(Î»_init))
    where_Î»_init_i = np.argwhere(np.iscomplex(Î»_init))

    return (lnÎ»_real_r, lnÎ»_real_init.imag, lnÎ»_comp_a, lnÎ»_comp_b, where_Î»_init_r, where_Î»_init_i), log_eig_constitute
  return params

def log_eig_constitute(lnÎ»_real_r, lnÎ»_real_i_init, lnÎ»_comp_a, lnÎ»_comp_b, where_Î»_init_r, where_Î»_init_i):
  k,n,_ = lnCÊ¹_r.shape
  lnÎ»_real = tf.complex(lnÎ»_real_r, lnÎ»_real_i_init)

  # Keep conjugate pairs adjacent [-b_1, b_1, -b_2, b_2, ...]
  lnÎ»_comp_r = tf.repeat(lnÎ»_comp_a, 2, axis=0)
  lnÎ»_comp_i = tf.repeat(lnÎ»_comp_b, 2, axis=0) * np.tile([-1,1], lnÎ»_comp_b.shape[0])
  lnÎ»_comp = tf.complex(lnÎ»_comp_r, lnÎ»_comp_i)
  # restore original order of eigenvalues
  lnÎ» = tf.scatter_nd(
              np.concatenate((where_Î»_init_r, where_Î»_init_i)),
              tf.concat([lnÎ»_real, lnÎ»_comp], axis=0),
              [k*n])    
  lnÎ» = tf.reshape(lnÎ», [k,n])
  Î» = tf.exp(lnÎ»)

  return lnÎ», Î»

def canonical_eig_param(n, m, k, useD=False):
  def params(n,m,k):
    a = tf.Variable(tf.random.normal((k,n)) / float(10000*n), name="a", dtype=tf.float32)
    return (a,), canonical_eig_constitute

def canonical_eig_constitute(a):
  k,n = a.shape
  A = np.diagflat(np.ones(shape=n-1), 1)[:-1]
  A = np.tile(A.reshape(1,n-1,n), (k,1,1))
  A = A.astype(np.float32)
  a_ = tf.expand_dims(a, 1)
  A = tf.concat([A, -a_], axis=1)
  Î» = tf.linalg.eigvals(A) #FIXME: double work
  lnÎ» = tf.math.log(Î»)
  return lnÎ», Î»

# Optimize over real C, potentially encountering numerical stability
def standard_out_param(C_init=None, D_init=None, Dâ‚’_init=None):
  def params(n,m,k):
    C = tf.Variable(tf.random.normal((k,n,m), stddev=0.00001), name="C", dtype=tf.float32)

    if D_init is None:
      D_init = np.random.uniform(low=-0.001, high=0.001, size=[k,m]).astype(np.float32) / m
    D = tf.Variable(D_init, name="D", dtype=tf.float32)
    if Dâ‚’_init is None:
      Dâ‚’_init = 0.0
    Dâ‚’ = tf.Variable(Dâ‚’_init, name="D0", dtype=tf.float32)  

    return (C, D, Dâ‚’), standard_out_constitute
  return params

def standard_out_constitute(lnÎ», Î», C, D, Dâ‚’):
  CÊ¹ = computeCÊ¹(lnÎ», Î», C) 
  return CÊ¹, D, Dâ‚’


# Expands optimization over real C to complex CÊ¹.
# After optimization, CU â‰ˆ CÊ¹ with equivalent loss can be found 
# Furthermore, optimize over lnCÊ¹ for numerical reasons
# (typically, C is very close to 0)
# Uses lnÎ», Î» for initial value 
def relaxed_out_param(useD=True):
  def params(n,m,k):
    lnCÊ¹_init = tf.math.log(tf.cast(tf.random.normal((k,n,m), mean=1.0, stddev=0.01), tf.complex64))
    lnCÊ¹_r = tf.Variable(tf.math.real(lnCÊ¹_init), name="lnC'_r", dtype=tf.float32)
    lnCÊ¹_i = tf.Variable(tf.math.imag(lnCÊ¹_init), name="lnC'_i", dtype=tf.float32)
    
    if useD:
      D_init = np.random.uniform(low=-0.001, high=0.001, size=[k,m]).astype(np.float32)
      Dâ‚’_init = np.random.uniform(low=-0.0000001, high=0.0000001, size=[m]).astype(np.float32)
    else:
      D_init = np.zeros([k,m], dtype=np.float32)
      Dâ‚’_init = np.zeros([m], dtype=np.float32)
    D = tf.Variable(D_init, name="D", dtype=tf.float32, trainable=useD)
    Dâ‚’ = tf.Variable(Dâ‚’_init, name="D0", dtype=tf.float32, trainable=useD) 

    return (lnCÊ¹_r, lnCÊ¹_i, D, Dâ‚’), relaxed_out_constitute
  return params

def relaxed_out_constitute(lnÎ», Î», lnCÊ¹_r, lnCÊ¹_i, D, Dâ‚’):
  lnCÊ¹ = tf.complex(lnCÊ¹_r, lnCÊ¹_i)
  CÊ¹ = tf.exp(lnCÊ¹)
  return CÊ¹, D, D

# Takes [batch size, T, d] real
# Returns [batch_size, T, m] real (in averaging mode)
#      or [batch_size, T, d, m]   (with k=None)
# Note: requires a fixed batch size. This means you must:
# 1. Use an InputLayer in the Keras model specification. 
# 2. Pass a TF dataset which has been batched with drop_remainder=True to model.fit(), 
#    not a NumPy array. (Otherwise the last batch will be the wrong size.)
class LDStack(tf.keras.layers.Layer):
  # n: state size
  # d: input size
  # m: output size
  # k: number of random projections (averaging mode)
  # Î”: depth of stack
  # standard: whether to compute 
  # last_only: return just the last element of the output sequence
  def __init__(self, n, d, m, k=None, Î”=1, eig_param=randroot_eig_param(), out_param=relaxed_out_param(), standard=True, last_only=False, num_stacks=1):
    super(LDStack, self).__init__()
    self.n = n
    self.m = m
    self.k = k
    self.Î” = Î”
    self.eig_param = eig_param
    self.out_param = out_param
    self.standard = standard
    self.average = k is not None
    self.last_only = last_only
    self.num_stacks = num_stacks

  def build(self, input_shape):
    self.b, self.T, d = input_shape
    print(input_shape, "input shape is")
    n, m, k, Î”, eig_param, out_param, standard, average, last_only, num_stacks = (self.n, self.m, self.k, self.Î”, self.eig_param, self.out_param, self.standard, self.average, self.last_only, self.num_stacks) 

    # Only perform random projection if number of projections is specified
    # Otherwise, run SIMO LDS on each coordinate of original input
    if average:
      self.R = tf.Variable(np.random.normal(size=(d,k)), name="R", dtype=tf.float32, trainable=False)
    else:
      k = d

    self.mid_layers = []
    # Performance optimization for sparse, purely-linear case 
    if Î” == 1 and num_stacks == 1 and last_only:
      self.last_layer = SparseLDS(n, m, k, init, average, standard)
    else:
      for i in np.arange(num_stacks-1):
        self.mid_layers.append( LDStackInner(n, k, k, Î”, eig_param, out_param, average, standard) )
      self.last_layer = LDStackInner(n, m, k, Î”, eig_param, out_param, average, standard, last_only=last_only)

  def call(self, x):
    if self.average:
      x = tf.tensordot(x, self.R, [[-1], [0]])

    x = tf.complex(tf.transpose(x, (1,0,2)), 0.0)
    for layer in self.mid_layers:
      x = layer(x)
      x = tf.reshape(x, (self.T, self.b, -1))
    y = tf.math.real(self.last_layer(x))

    # Return to batch-major shape 
    if self.last_only:
      # time axis already eliminated when last state was taken
      return y
    else:
      degree = len(y.shape)
      return tf.transpose(y, (1,0,2,3)) if degree == 4 else tf.transpose(y, (1,0,2))


def get_init_and_const_funcs(init):
  if init == 'unitary':
    return unitary_initialization, unitary_constitute
  elif init == 'randroot':
    return randroot_initialization, fixed_constitute
  elif init == 'canonical':
    return canonical_initialization, canonical_constitute

# Average of k SIMO LDS, only returning last state. Uses much more memory-efficient SparseLinearRecurrence op
class SparseLDS(tf.keras.layers.Layer):
  def __init__(self, n, m, k, eig_param, out_param, average, standard=True):
    super(SparseLDS, self).__init__()
    self.eig_underlying, self.constitute_lnÎ»_Î» = eig_param(n, m, k)
    self.out_underlying, self.constitute_CÊ¹_D_Dâ‚’ = out_param(n, m, k)
    self.k = k
    self.n = n
    self.m = m 
    self.average = average
    self.standard = standard

  def call(self, x):
    T, b, k = x.shape
    n = self.n
    lnÎ», Î» = self.constitute_lnÎ»_Î»(*self.eig_underlying)
    CÊ¹, D, Dâ‚’ = self.constitute_CÊ¹_D_Dâ‚’(*((lnÎ», Î») + self.out_underlying))
    BÊ¹ = computeBÊ¹(lnÎ», Î»)

    # linear_recurrence computes sÊ¹_t = Î»Â·sÊ¹_{t-1} + Bx_t
    # for standard LDS, we need to shift x
    # sÊ¹_t = Î»Â·sÊ¹_{t-1} + Bx_{t-1}
    if self.standard:
      x = tf.concat([tf.zeros((1,b,k), dtype=tf.complex64),  x[:-1]], axis=0)
    # [b,k,n]
    sâ‚œÊ¹ = sparse_linear_recurrence(Î», x, BÊ¹) 
    # sâ‚œÊ¹: [b, k, n]
    # CÊ¹: [k, n, m]    
    # [b, k, n, 1] * [1, k, n, m] -> [b, k, m]
    CÊ¹Â·sâ‚œÊ¹ = tf.reduce_sum(tf.expand_dims(sâ‚œÊ¹, -1)*tf.expand_dims(CÊ¹,0), axis=-2)
    DÂ·xâ‚œ = tf.expand_dims(x[-1], -1) * tf.complex(tf.expand_dims(D, 0), 0.0)
    yâ‚œ = CÊ¹Â·sâ‚œÊ¹ + DÂ·xâ‚œ + tf.complex(Dâ‚’, 0.0)
    
    if self.average:
      yâ‚œ = tf.reduce_mean(yâ‚œ, axis=1) 
    return yâ‚œ

  # TODO: (A, B, C, D) by computing elementary symmetric polynomials 
  def canonical_lds():
    return None

# Full generality, but slower
class LDStackInner(tf.keras.layers.Layer):
  def __init__(self, n, m, k, Î”, eig_param, out_param, average, standard=True, last_only=False):
    super(LDStackInner, self).__init__()
    self.eig_underlying, self.constitute_lnÎ»_Î» = eig_param(n, m, k)
    self.out_underlying, self.constitute_CÊ¹_D_Dâ‚’ = out_param(n, m, k)
    self.n = n
    self.m = m 
    self.k = k
    self.Î” = Î”
    self.average = average
    self.standard = standard
    self.last_only = last_only

  def build(self, input_shape):
    self.T, self.b, _ = input_shape 

  # x : [T, b, k]
  # Î» : [k, n]
  # C : [k, m, n]
  # D : [k, m]
  # Î± : [T, b, k, n] is:
  #      Î±_0, ..., Î±_{T-1} (in "standard" mode)
  #      Î±_1, ..., Î±_T     (otherwise)
  # Returns (for t in [1,...,T]):
  # sÊ¹: [T, b, k, n] is:
  #     sÊ¹_t = Î±_{t-1}Â·Î»Â·sÊ¹_{t-1}  + Bx_{t-1}   (in "standard" mode)
  #     sÊ¹_t = Î±_t    Â·Î»Â·sÊ¹_{t-1}  + Bx_t       (otherwise)
  # currently with sÊ¹_0 = 0
  # y : [T, b, k, m] is:
  #     y_t  = CÊ¹sÊ¹_t + Dx_t
  # y returned only if C, D, Dâ‚’ are provided

  def batch_simo_lds(self, x, lnÎ», Î», CÊ¹=None, D=None, Dâ‚’=None, Î±=None, standard=True):
    k, n, m, T, b = (self.k, self.n, self.m, self.T, self.b)
    states_only = CÊ¹ is None or D is None or Dâ‚’ is None
    BÊ¹ = computeBÊ¹(lnÎ», Î»)

    # BÊ¹ : [k, n]  
    # BÊ¹Â·x : [T, b, k*n]   
    BÊ¹Â·x = tf.reshape(tf.expand_dims(x, -1)*BÊ¹, (T, b, k*n))
    
    # Î±Â·Î» : [T, b, k*n]
    # sÊ¹ : [T, b, k, n]  
    if Î± is None:
      Î±Â·Î» =  tf.tile(tf.reshape(Î», (1,1,-1)), (T, b, 1)) 
    else:
      Î±Â·Î» = Î±*tf.reshape(Î», (1, 1, k, n))  
      Î±Â·Î» = tf.reshape(Î±Â·Î», (T, b, k*n))
    # linear_recurrence computes sÊ¹_t = Î±_tÂ·Î»Â·sÊ¹_{t-1} + Bx_t
    # for standard LDS, we need to shift Î± and x
    # sÊ¹_t = Î±_{t-1}Â·Î»Â·sÊ¹_{t-1} + Bx_{t-1}
    if standard:
      Î±Â·Î»  = tf.concat([tf.zeros((1,b,k*n), dtype=tf.complex64),  Î±Â·Î»[:-1]], axis=0)
      BÊ¹Â·x = tf.concat([tf.zeros((1,b,k*n), dtype=tf.complex64),  BÊ¹Â·x[:-1]], axis=0)
    sÊ¹ = linear_recurrence(Î±Â·Î», BÊ¹Â·x)
    sÊ¹ = tf.reshape(sÊ¹, [T, b, k, n])
    if states_only:
      return sÊ¹
      
    # sÊ¹ : [T,b,k,n]
    # CÊ¹ : [k,n,m] 
    # [T,b,k,n,1] * [1,1,k,n,m] -> [T,b,k,m]
    CÊ¹Â·sÊ¹ = tf.reduce_sum(tf.expand_dims(sÊ¹, -1)*tf.reshape(CÊ¹, (1,1,k,n,m)), axis=-2)
    DÂ·x = tf.expand_dims(x, -1) * tf.complex(tf.reshape(D, (1,1,k,m)), 0.0)
    y = CÊ¹Â·sÊ¹ + DÂ·x + tf.complex(Dâ‚’, 0.0)

    return sÊ¹, y

  # x: [T, batch size, k] is complex (this is unusual, but matches the format of linear_recurrence.
  #       And actually, is faster for GPU computation, and should be the standard for RNNs.)
  # Returns complex output y of shape [T, batch size, m]
  def call(self, x):
    T, b, k = x.shape
    n = self.n
    lnÎ», Î» = self.constitute_lnÎ»_Î»(*self.eig_underlying)
    CÊ¹, D, Dâ‚’ = self.constitute_CÊ¹_D_Dâ‚’(*((lnÎ», Î») + self.out_underlying))

    # Î» : [k, n]
    # Î± : [T, b, k, n]
    # sÊ¹: [T, b, k, n]
    # y : [T, b, k, m]
    Î±=None
    for i in np.arange(self.Î” - 1):
      sÊ¹ = self.batch_simo_lds(x, lnÎ», Î», Î±=Î±, standard=self.standard)
      Î»Â·sÊ¹ = tf.reshape(Î», (1,1,k,n)) * sÊ¹
      Î± = recipsq(Î»Â·sÊ¹)
    _, y = self.batch_simo_lds(x, lnÎ», Î», CÊ¹, D, Dâ‚’, Î±, self.standard)
    if self.average:
      y = tf.reduce_mean(y, axis=2)
    if self.last_only:
      y = y[-1]
    return y


# Careful computation of numerically unstable quantities
def computeBÊ¹(lnÎ», Î»):
  k, n = lnÎ».shape
  
  # ratios[k,i,j] = Î»i / Î»j for k'th Î»
  ratios = tf.reshape(Î», (k, -1, 1)) / tf.reshape(Î», (k, 1, -1))
  #ln_ratios = tf.reshape(lnÎ», (k, -1, 1)) - tf.reshape(lnÎ», (k, 1, -1))
  ratios = tf.linalg.set_diag(ratios, tf.zeros(shape=[k,n], dtype=tf.complex64))

  # BÊ¹_i = Î»i^{n-1} / âˆ_{iâ‰ j} Î»i-Î»j
  # log BÊ¹_i 
  # = (n-1) logÎ»i - âˆ‘_{iâ‰ j} log(Î»i-Î»j)
  # = (n-1) logÎ»i - âˆ‘_{iâ‰ j} logÎ»i + log(1 - Î»j/Î»i)
  # = -âˆ‘_{iâ‰ j} log(1 - Î»j/Î»i)
  # = -âˆ‘_j log(1 - ratios[k,j,i]) 
  # because log(1 - ratios[k,i,i]) = 0
  lnBÊ¹ = -1* tf.reduce_sum(tf.math.log1p(-ratios), axis=1)
  # BÊ¹ : [k, n]
  BÊ¹ = tf.exp(lnBÊ¹)
  return BÊ¹


def computeCÊ¹(lnÎ», Î», C, first_method=False): 
  n = Î».shape[-1]

  if first_method:
    # Avoids taking log(C), but does need to take log(CÂ·Î»â±)
    # BROKEN
    Î»â± = tf.expand_dims(tf.math.pow(tf.expand_dims(Î», -1), tf.cast(tf.range(1,n+1), dtype=tf.complex64)), 0) # [1, k, n, n]  
    C = tf.expand_dims(tf.transpose(tf.cast(C, tf.complex64), (1,0,2)), 2) # [m, k, 1, n] 
    CÂ·Î»â± = tf.reduce_sum(C*Î»â±, axis=-1) # [m, k, n]  
    CÊ¹ = tf.exp(tf.cast(-n, tf.complex64)*tf.expand_dims(lnÎ», 0) + tf.math.log(CÂ·Î»â±)) # [1,k,n]+[m,k,n]
    CÊ¹ = tf.transpose(CÊ¹, (1,2,0)) # [k, n, m]
  else:
    # Takes log(C)
    lnC = tf.transpose(tf.math.log(tf.complex(C, 0.0)), (2,0,1)) # [m, k, n]
    p = tf.cast(n - tf.range(1,n+1), tf.complex64)
    lnU = tf.reshape(-1*p, (1,-1,1)) * tf.expand_dims(lnÎ», 1) # [1,n,1]*[k,1,n] -> [k, n, n] 
    lnC_lnU = tf.expand_dims(lnC,-1) + tf.expand_dims(lnU, 0) # [m,k,n,1]+[1,k,n,n] -> [m, k, n, n]
    CÊ¹ = tf.reduce_sum(tf.exp(lnC_lnU), axis=-2) # [m, k, n]
    CÊ¹ = tf.transpose(CÊ¹, (1,2,0)) # [k, n, m] 
  tf.debugging.check_numerics(tf.math.real(CÊ¹), message="C'")
  return CÊ¹

# Reciprocal square root nonlinearity
def recipsq(a):
  return tf.complex(tf.math.rsqrt(1 + tf.math.square(tf.abs(a))), 0.0)
