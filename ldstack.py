import numpy as np
import tensorflow as tf
from linear_recurrent_net.linear_recurrent_net.tensorflow_binding import linear_recurrence

# Takes [batch size, T, d] real
# Returns [batch_size, T, m] real
# Note: requires a fixed batch size. This means you must:
# 1. Use an InputLayer in the Keras model specification. 
# 2. Pass a TF dataset which has been batched with drop_remainder=True to model.fit(), 
#    not a NumPy array. (Otherwise the last batch will be the wrong size.)
class LDStack(tf.keras.layers.Layer):
  # n: state size
  # d: input size
  # m: output size
  # k: number of random projections
  # Î”: depth of stack
  # standard: whether to compute 
  # last_only: return just the last element of the output sequence
  def __init__(self, n, d, m, k, Î”, init='unitary', standard=True, last_only=False):
    super(LDStack, self).__init__()
    self.ldstack = LDStackInner(n, d, m, k, Î”, init, standard)
    self.last_only = last_only

  def call(self, x):
    x = tf.complex(tf.transpose(x, (1,0,2)), 0.0)
    y = tf.math.real(self.ldstack(x))
    if self.last_only:
      y = y[-1,:]
    else:
      y = tf.transpose(y, (1,0,2))
    return y


# Takes time-major complex sequences, which is performant, but not compatible with Keras
class LDStackInner(tf.keras.layers.Layer):
  def __init__(self, n, d, m, k, Î”, init='unitary', standard=True):
    super(LDStackInner, self).__init__()
    
    # FIXME
    #if d > 1 or k > 1:
    self.R = tf.Variable(np.random.normal(size=(d,k)), name="R", dtype=tf.float32, trainable=False)
    
    if init == 'unitary':
      (lnÎ», C, D, Dâ‚’), (lnÎ»_init, C_init, D_init, Dâ‚’_init) = self.unitary_initialization(m, n, k)
    elif init == 'randroots':
      (lnÎ», C, D, Dâ‚’), (lnÎ»_init, C_init, D_init, Dâ‚’_init) = self.randroot_or_fixed_initialization(m, n, k)
    elif isinstance(init, tuple):
      (Î»_init, C_init, D_init, Dâ‚’_init) = init
      lnÎ», C, D, Dâ‚’ = self.randroot_or_fixed_initialization(m, n, k, Î»_init, C_init, D_init, Dâ‚’_init)

    self.lnÎ» = lnÎ»
    self.C = C
    self.D = D
    self.Dâ‚’ = Dâ‚’
    self.k = k
    self.n = n
    self.Î” = Î”
    self.standard = standard

  # Initialize with uniformly random complex eigenvalues of magnitude 1.
  # If Î» has polar coordinates (r, Î¸) then ln(Î») = ln(r) + Î¸i
  # Also, ln(Î»*) = ln(r) - Î¸i
  # Fix r=1, i.e. ln(r) = 0. (For numerical reasons, can fix r=1-ð›¿ for some small ð›¿) 
  # Note this only requires n/2 parameters rather than n
  # Should constrain -Ï€ â‰¤ Î¸ â‰¤ Ï€
  def unitary_initialization(self, m, n, k, trainÎ»=True):
    if n % 2 != 0:
      raise "n must be even"

    half_n = round(n/2)
    Î¸_init = np.random.uniform(low=-np.pi, high=np.pi, size=[k,half_n]).astype(np.float32)
    Î¸ = tf.Variable(Î¸_init, name="eig_angle", dtype=tf.float32, trainable=trainÎ»)
    lnÎ»_r = tf.zeros((k,n), dtype=tf.float32)
    #lnÎ»_r = np.log(1-ð›¿)*tf.ones((k,n), dtype=tf.float32)
      
    lnÎ»_i = tf.concat([Î¸, -Î¸], axis=1)
    lnÎ» = tf.complex(lnÎ»_r, lnÎ»_i)
    lnÎ»_init = 0 #FIXME

    C_init = np.random.uniform(low=-0.000001, high=0.000001, size=[k,m,n]).astype(np.float32) / n
    C = tf.Variable(C_init, name="C", dtype=tf.float32)
    D_init = np.random.uniform(low=-0.001, high=0.001, size=[k,m]).astype(np.float32)
    D = tf.Variable(D_init, name="D", dtype=tf.float32)
    Dâ‚’_init = np.random.uniform(low=-0.0000001, high=0.0000001, size=[m]).astype(np.float32)
    Dâ‚’ = tf.Variable(Dâ‚’_init, name="D0", dtype=tf.float32)    
      
    return (lnÎ», C, D, Dâ‚’), (lnÎ»_init, C_init, D_init, Dâ‚’_init)

  # Initialization as roots of monic polynomial with random coefficients
  def randroot_or_fixed_initialization(self, m, n, k, Î»_init=None, C_init=None, D_init=None, Dâ‚’_init=None, stable=True):
    if Î»_init is None:
      Î»_init = np.zeros((k,n), dtype=np.complex64)
      for i in np.arange(k):
        Ainit = np.diagflat(np.ones(shape=n-1), 1).astype(np.float32)
        Ainit[-1,:] = np.random.normal(size=n) / n
        Î»_init[i] = np.linalg.eigvals(Ainit)

      if stable:
        Î»_init = Î»_init / np.maximum(1.0, np.abs(Î»_init))
    Î»_init = Î»_init.flatten()
    
    # isolated (non paired) log eigenvalue ends up real if imaginary part is either 0 or pi 
    # only optimize over real part, fix the imaginary part. this ensures eigenvalue is real, but fixes its sign
    # (which is OK, because it shouldn't approach zero anyway.)
    lnÎ»_real_init = np.log(Î»_init[np.isreal(Î»_init)] + 0j)
    lnÎ»_real_r = tf.Variable(lnÎ»_real_init.real, dtype=tf.float32)
    lnÎ»_real = tf.complex(lnÎ»_real_r, lnÎ»_real_init.imag)

    comp_pair = (Î»_init[np.iscomplex(Î»_init)])[::2] # only get one part of conjugate pair.
    ln_comp_pair = np.log(comp_pair)
    lnÎ»_comp_init_a = ln_comp_pair.real.astype(np.float32)
    lnÎ»_comp_init_b = ln_comp_pair.imag.astype(np.float32)
              
    lnÎ»_comp_a = tf.Variable(lnÎ»_comp_init_a, dtype=tf.float32)
    lnÎ»_comp_b = tf.Variable(lnÎ»_comp_init_b, dtype=tf.float32)
    # Keep conjugate pairs adjacent [-b_1, b_1, -b_2, b_2, ...]
    # tf.repeat() not in 1.13
    lnÎ»_comp_r = tf.keras.backend.repeat_elements(lnÎ»_comp_a, 2, axis=0)
    lnÎ»_comp_i = tf.keras.backend.repeat_elements(lnÎ»_comp_b, 2, axis=0) * np.tile([-1,1], lnÎ»_comp_init_b.shape[0])
    lnÎ»_comp = tf.complex(lnÎ»_comp_r, lnÎ»_comp_i)
    # restore original order of eigenvalues
    # TF BUG in scatter_nd() seems to mangle complex values 
    if False:
      lnÎ» = tf.scatter_nd(
                np.concatenate((np.argwhere(np.isreal(Î»_init)), np.argwhere(np.iscomplex(Î»_init)))),
                tf.concat([lnÎ»_real, lnÎ»_comp], axis=0),
                [k*n])    
    else:
      lnÎ»_r = tf.scatter_nd(
                  np.concatenate((np.argwhere(np.isreal(Î»_init)), np.argwhere(np.iscomplex(Î»_init)))),
                  tf.math.real(tf.concat([lnÎ»_real, lnÎ»_comp], axis=0)),
                  [k*n]) 
      lnÎ»_i = tf.scatter_nd(
                  np.concatenate((np.argwhere(np.isreal(Î»_init)), np.argwhere(np.iscomplex(Î»_init)))),
                  tf.math.imag(tf.concat([lnÎ»_real, lnÎ»_comp], axis=0)),
                  [k*n]) 
      lnÎ» = tf.complex(lnÎ»_r, lnÎ»_i)
    lnÎ» = tf.reshape(lnÎ», [k,n])

    if C_init is None:
      C_init = np.random.uniform(low=-0.000001, high=0.000001, size=[k,m,n]).astype(np.float32) / n**2
    C = tf.Variable(C_init, dtype=tf.float32)
    if D_init is None:
      D_init = np.random.uniform(low=-0.001, high=0.001, size=[k,m]).astype(np.float32) / m
    D = tf.Variable(D_init, dtype=tf.float32)
    if Dâ‚’_init is None:
      Dâ‚’_init = 0.0
    Dâ‚’ = tf.Variable(Dâ‚’_init, dtype=tf.float32)  

    return (lnÎ», C, D, Dâ‚’), (np.log(Î»_init), C_init, D_init, Dâ‚’_init) 

  # x : [T, b, k]
  # Î» : [k, n]
  # C : [k, m, n]
  # D : [k, m]
  # Î± : [T, b, k, n] is:
  #      Î±_0, ..., Î±_{T-1} (in "standard" mode)
  #      Î±_1, ..., Î±_T     (otherwise)
  # Returns (for t in [1,...,T]):
  # sÊ¹: [T, b, k, n] is:
  #     sÊ¹_t = Î±_{t-1}Â·Î»Â·sÊ¹_{t-1}  + Bx_{t-1}   (in "standard" mode)
  #     sÊ¹_t = Î±_t    Â·Î»Â·sÊ¹_{t-1}  + Bx_t       (otherwise)
  # currently with sÊ¹_0 = 0
  # y : [T, b, k, m] is:
  #     y_t  = CÊ¹sÊ¹_t + Dx_t
  def batch_simo_lds(self, x, lnÎ», C, D, Dâ‚’, Î±=None, standard=False):
    k, n = (self.k, self.n)
    T, b, d = x.shape
    m = C.shape[1]

    # ratios[k,i,j] = Î»i / Î»j for k'th Î»
    Î» = tf.exp(lnÎ»)
    ratios = tf.reshape(Î», (k, -1, 1)) / tf.reshape(Î», (k, 1, -1))
    #ratios = tf.exp(tf.reshape(lnÎ», (k, -1, 1)) - tf.reshape(lnÎ», (k, 1, -1)))
    ratios = tf.linalg.set_diag(ratios, tf.zeros(shape=[k,n], dtype=tf.complex64))
    
    # BÊ¹_i = Î»i^{n-1} / âˆ_{iâ‰ j} Î»i-Î»j
    # log BÊ¹_i 
    # = (n-1) logÎ»i - âˆ‘_{iâ‰ j} log(Î»i-Î»j)
    # = (n-1) logÎ»i - âˆ‘_{iâ‰ j} logÎ»i + log(1 - Î»j/Î»i)
    # = -âˆ‘_{iâ‰ j} log(1 - Î»j/Î»i)
    # = -âˆ‘_j log(1 - ratios[k,j,i]) 
    # because log(1 - ratios[k,i,i]) = 0
    lnBÊ¹ = -1* tf.reduce_sum(tf.math.log(1.0 - ratios), axis=1)
    # BÊ¹ : [k, n]
    # BÊ¹Â·x : [T, b, k*n]
    BÊ¹ = tf.exp(lnBÊ¹)
    BÊ¹Â·x = tf.reshape(tf.expand_dims(x, -1)*BÊ¹, (T, b, k*n))
    
    # Î±Â·Î» : [T, b, k*n]
    # sÊ¹ : [T, b, k, n]  
    if Î± is None:
      Î±Â·Î» =  tf.tile(tf.reshape(Î», (1,1,-1)), (T, b, 1)) 
    else:
      Î±Â·Î» = Î±*tf.reshape(Î», (1, 1, k, n))  
      Î±Â·Î» = tf.reshape(Î±Â·Î», (T, b, k*n))
    # linear_recurrence computes sÊ¹_t = Î±_tÂ·Î»Â·sÊ¹_{t-1} + Bx_t
    # for standard LDS, we need to shift Î± and x
    # sÊ¹_t = Î±_{t-1}Â·Î»Â·sÊ¹_{t-1} + Bx_{t-1}
    if standard:
      Î±Â·Î»  = tf.concat([tf.zeros((1,b,k*n), dtype=tf.complex64),  Î±Â·Î»[:-1]], axis=0)
      BÊ¹Â·x = tf.concat([tf.zeros((1,b,k*n), dtype=tf.complex64),  BÊ¹Â·x[:-1]], axis=0)
    sÊ¹ = linear_recurrence(Î±Â·Î», BÊ¹Â·x)
    sÊ¹ = tf.reshape(sÊ¹, [T, b, k, n])
      
    # Numerically stable calculation of CÊ¹ = CU
    Î»â± = tf.expand_dims(tf.math.pow(tf.expand_dims(Î», -1), tf.cast(tf.range(1,n+1), dtype=tf.complex64)), 0) # [1, k, n, n]
    tf.debugging.check_numerics(tf.math.real(Î»â±), message='lampow nan')
    C = tf.expand_dims(tf.transpose(tf.cast(C, tf.complex64), (1,0,2)), 2) # [m, k, 1, n]
    CÂ·Î»â± = tf.reduce_sum(C*Î»â±, axis=-1) # [m, k, n]
    CÊ¹ = tf.exp(tf.math.log(tf.cast(-n, tf.complex64)*tf.expand_dims(lnÎ», 0)) + tf.math.log(CÂ·Î»â±))
    CÊ¹ = tf.transpose(CÊ¹, (1,2,0)) # [k, n, m]
    tf.debugging.check_numerics(tf.math.real(CÊ¹), message="C'")
      
    CÊ¹Â·sÊ¹ = tf.reduce_sum(tf.expand_dims(sÊ¹, -1)*tf.reshape(CÊ¹, (1,1,k,n,m)), axis=-2)
    DÂ·x = tf.expand_dims(x, -1) * tf.complex(tf.reshape(D, (1,1,k,m)), 0.0)
    y = CÊ¹Â·sÊ¹ + DÂ·x + tf.complex(Dâ‚’, 0.0)

    return sÊ¹, y

  # x: [T, batch size, d] is complex (this is unusual, but matches the format of linear_recurrence.
  #       And actually, is faster for GPU computation, and should be the standard for RNNs.)
  # Returns complex output y of shape [T, batch size, m]
  def call(self, x):
    T, b, d = x.shape
    k, n = (self.k, self.n)

    if d > 1 or k > 1:
      R = tf.cast(self.R, tf.complex64)
      x = tf.tensordot(x, R, [[-1], [0]])
      
    Î» = tf.exp(self.lnÎ»)
    # Î» : [k, n]
    # C : [k, m, n]
    # Î± : [T, b, k, n]
    # sÊ¹: [T, b, k, n]
    # y : [T, b, k, m]
    Î± = None
    for i in np.arange(self.Î”):
      sÊ¹, y = self.batch_simo_lds(x, self.lnÎ», self.C, self.D, self.Dâ‚’, Î±, self.standard)
      Î»Â·sÊ¹ = tf.reshape(Î», (1,1,k,n)) * sÊ¹
      Î± = recipsq(Î»Â·sÊ¹)

    y = tf.reduce_mean(y, axis=2)
    return y

  # TODO: (A, B, C, D) by computing elementary symmetric polynomials 
  def canonical_lds():
    if self.Î” > 1:
      raise "LDStack with Î” > 1 does not represent an LDS"
    return None

# Reciprocal square root nonlinearity
def recipsq(a):
  return tf.complex(tf.math.rsqrt(1 + tf.math.square(tf.abs(a))), 0.0)
